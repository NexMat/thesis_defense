<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
		<title>Novel deep learning approaches for image analysis</title>
		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/custom.css">
		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-transition="fade-out" class="center" style="color: #254456;">
					<div style="width: 100%;">
						<div style="float:left; width: 35%; margin-top: 10%;">
							<img src="res/za/logo_opis.png" alt="Logos OPIS">
						</div>
						<div style="float:right; width: 64%;">
							<h1>Novel Deep Learning Approaches For Image Analysis</h1>
							<h2>Mathieu Vu</h2>
							<em>Ph.D. defense - 28th November 2024</em>
							<table class="left_aligned" style="margin-top:5%;">
								<tr>
									<td><em>Supervisors:</em></td>
								</tr>
								<tr>
									<td><b>√âmilie Chouzenoux</b></td>
									<td>Research Director (Inria, France)</td>
								</tr>
								<tr>
									<td><b>Philippe Pinault</b></td>
									<td>R&D Engineer (EssilorLuxottica, France)</td>
								</tr>
								<tr>
									<td style="opacity: 0%;">-</td>
								</tr>
								<tr>
									<td><em>Jury members:</em></td>
								</tr>
								<tr>
									<td><b>Amel Benazza</b></td>
									<td>Professor (SUP‚ÄôCOM Tunis, Tunisia)</td>
								</tr>
								<tr>
									<td><b>Adrian Basarab</b></td>
									<td>Professor (Universit√© de Lyon, France)</td>
								</tr>
								<tr>
									<td><b>Benjamin Guedj</b></td>
									<td>Research Director (Inria, UCL, United Kingdom)</td>
								</tr>
								<tr>
									<td><b>C√©line Hudelot</b></td>
									<td>Professor (CentraleSup√©lec, France)</td>
								</tr>
								<tr>
									<td><b>Ismail Ben Ayed</b></td>
									<td>Professor (√âcole de Technologie Sup√©rieure, Canada)</td>
								</tr>
							</table>
						</div>
					</div>
					<aside class="notes">
						Good morning everyone, I am pleased to welcome you at my Ph.D. defense where I'll be presenting my thesis titled Novel Deep Learning Approaches for Images Analysis.<br /><br />
						This work was done in the OPIS Inria team, at the Centre for Visual Computing, here at CentraleSup√©lec under the supervision of √âmilie Chouzenoux.<br /><br />
						We also collaborated with Essilor Research and Development department, with Philippe Pinault who co-supervised this thesis. Let's start with the summary of this presentation.
					</aside>
				</section>
				<section data-auto-animate data-transition="fade-in none-out" class="section_slide">
					<h1>Summary</h1>
					<div class="header_hr" data-id="header_hr">
						<hr />
					</div>
					<div style="width: 100%;">
						<div style="float:left; width: 50%;" data-id="left_col">
							<h1 class="fragment fade-down" data-fragment-index="1" style="font-size: 1.6em;">Part I - Ensemble learning</h1>
							<ol>
								<li class="fragment fade-down" data-fragment-index="2">
									Introduction
									<ol>
										<li>Wisdom of the crowd</li>
										<li>Ensemble learning</li>
										<li>Generalised mean</li>
									</ol>
								</li>
								<li class="fragment fade-down" data-fragment-index="3">
									Proposed model
									<ol>
										<li>$f$-average</li>
										<li>Aggregated $f$-averages</li>
										<li>Stacked aggregated $f$-averages</li>
										<li>Application example</li>
									</ol>
								</li>
								<li class="fragment fade-down" data-fragment-index="4">
									Application to Few-Shot Class Incremental Learning
									<ol>
										<li>Problem setting</li>
										<li>Proposed approach</li>
										<li>Comparative results</li>
									</ol>
								</li>
							</ol>
						</div>
						<div style=" float:right; width: 50%;">
							<h2 class="fragment fade-down" data-fragment-index="1">Part II - Deep learning for photorefraction</h2>
							<ol>
								<li class="fragment fade-down" data-fragment-index="2">
									Introduction
									<ol>
										<li>Context</li>
										<li>Objectives</li>
										<li>Eye refractive error</li>
									</ol>
								</li>
								<li class="fragment fade-down" data-fragment-index="3">
									Proposed pipeline
									<ol>
										<li>Photorefraction principle</li>
										<li>Essilor capture device</li>
										<li>Datasets, CNN architecture & training</li>
									</ol>
								</li>
								<li class="fragment fade-down" data-fragment-index="4">
									Implementation & performance
									<ol>
										<li>Regression models benchmark</li>
										<li>Real versus synthetic data</li>
										<li>Performance comparison with other device</li>
										<li>Implementation performance</li>
										<li>Ensembling regression models</li>
									</ol>
								</li>
								<li class="fragment fade-down" data-fragment-index="4">
									Robustness analysis
									<ol>
										<li>Numerical evaluation of robustness</li>
										<li>Improve robustness with a denoiser</li>
										<li>Controlled Lipschitz constant training</li>
									</ol>
								</li>
							</ol>
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						We will divide the content of this presentation into two main parts<br /><br />
						‚û°Ô∏è the first being about ensemble learning and the second about a deep learning approach for photorefraction.<br /><br />
						‚û°Ô∏èFor both of those parts, we will first have an introduction section,<br /><br />
						‚û°Ô∏èthen in a second section, we will present the details of our proposed approach,<br /><br />
						‚û°Ô∏èand finally in the last section, we will evaluate and compare their respective performance.
					</aside>
				</section>
				<section data-auto-animate data-transition="none-in slide-out" data-auto-animate-unmatched="false" class="section_slide">
					<h1>Part I - Ensemble learning</h1>
					<div class="header_hr" data-id="header_hr">
						<hr />
					</div>
					<div style="width: 100%;">
						<div style="float:left; width: 50%;" data-id="left_col">
							<h2 style="opacity: 0%;">.</h2>
							<ol>
								<li>
									Introduction
									<ol>
										<li>Wisdom of the crowd</li>
										<li>Ensemble learning</li>
										<li>Generalised mean</li>
									</ol>
								</li>
								<span class="semi-fade">
									<li>
										Proposed model
										<ol>
											<li>$f$-average</li>
											<li>Aggregated $f$-averages</li>
											<li>Stacked aggregated $f$-averages</li>
											<li>Application example</li>
										</ol>
									</li>
									<li>
										Application to Few-Shot Class Incremental Learning
										<ol>
											<li>Motivation & problem setting</li>
											<li>Proposed approach</li>
											<li>Comparative results</li>
										</ol>
									</li>
								</span>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr class="footer_hr" data-id="footer_hr" />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Let's start with the first section of our first part: we will introduce the basic principles of ensemble learning as well as what a generalised mean is.
					</aside>
				</section>
				<section class="section_slide">
					<h1>Introduction: wisdom of the crowd</h1>
					<hr />
					<div class="centred_img">
						<img src="res/hoxton_market_shoreditch_1910.jpg" style="width:45%" />
						<div>British market, 1910</div>
					</div>
					<div style="margin-top: 0%;">
						Francis Galton's study of a weight-guessing contest <span id="ref">[Galton, 1907]</span>: <br />
						<ul>
							<li>800 participants</li>
							<li>guesses mean: 1,208 pounds</li>
							<li>ox weight: 1,197 pounds</li>
						</ul>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						In the beginning of the 20th century, a contest was held in a British market in the coastal town of Plymouth. Participants, which could be anybody, had to guess visually the weight of an ox that was presented before them. Then, they had to write down their guess on a piece of paper in the hopes of winning some prize.<br /><br />
						Francis Galton did a statistical study of the answers and found out that the average guess was less than one percent more than the real weight. This is a classic example of how accurate the wisdom of the crowd, even of non-experts, can be.
					</aside>
				</section>
				<section class="section_slide">
					<h1>Introduction: ensemble learning</h1>
					<hr />
					<div class="centred_img">
						<img src="res/ensemble_learning.jpg" style="width:55%" />
						<div>where $(\forall k \in \{1,\ldots, K\}), x_k \in \mathbb{R}^N$ and $\widetilde{x} \in
							\mathbb{R}^N$
						</div>
					</div>
					<div style="margin-top: 4%;">
						<b>Goal üéØ</b>: leverage multiple models to enhance performance on a specific task <br />
						<ul>
							<li><b>Weak learners training</b>: get an ensemble of diverse models (eg. bagging, boosting etc.)</li>
							<li><b>Output fusion</b>: gather weak learners prediction to obtain the final unified prediction (eg. averaging, stacking etc.)</li>
						</ul>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Since then, this principle has been applied in many fields, including in machine learning, under the name of ensemble learning. The goal is to leverage an ensemble of models, for example using K models here, called weak learners instead of using a single model. These models predictions x_k (for example predicted logits in the case of classification) are then gathered in a step called output fusion in order to obtain the final unified prediction. So te be clear, the K predictions x k are the output of the respective K weak learners but are also the input of the output fusion step. It has been shown that such methods could decrease bias as well as increase accuracy and robustness. <br /><br />
						
						Some methods such as bagging or boosting focus on training an ensemble of diverse models, which is a key characteristic needed in ensemble learning.<br /><br />
						
						In our case, we will assume that we already have such set of diverse weak learners and we will focus on the output fusion part.<br /><br />
						
						In the state of the art, the most commonly used method to perform the output fusion step is either to average the outputs x_k or to train a stacked model, which is an additional machine learning model that learns to output the final prediction given the outputs x_k from the K weak learners. The method that we propose and that we will present today gathers these two approaches in a single model.
					</aside>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Introduction: generalised average</h1>
					<hr />
					<div>
						Consider $K$ estimates $(x_k)_{1 \le k \le K}$ and let $\widetilde{x}$ in $[0, +\infty[$ be the predicted output after ensembling. <br />
					</div>
					<div style="margin-top: 1%;">
						<b>Standard average rules:</b> <br />
						<table style="margin: 0;" class="table_means">
							<tr>
								<td>Arithmetic:</td>
								<td>$\widetilde{x} = \sum_{k=1}^K \omega_{k}x_{k}$</td>
							</tr>
							<tr>
								<td>Geometric:</td>
								<td>$\widetilde{x} = \prod_{k=1}^K x_{k}^{\omega_{k}}$</td>
							</tr>
							<tr>
								<td>Harmonic:</td>
								<td>$\widetilde{x} = (\sum_{k=1}^K \frac{\omega_{k}}{x_{k}})^{-1}$</td>
							</tr>
							<tr>
								<td>Power-$q$:</td>
								<td>$\widetilde{x} = \left( \sum_{k=1}^K \omega_{k} x_{k}^q \right)^{1/q}$</td>
							</tr>
						</table>
						where $(\omega_k)_{1 \le k \le K}$ are some weights such that $\sum_{k=1}^K \omega_k = 1$.
					</div>
					<div class="theorem">
						<div class="theorem_title">Generalised average <span id="ref">[Kolmogorov, 1930]</span></div>
						$\widetilde{x} = f^{-1}\Big(\sum_{k=1}^K \omega_{k} f(x_{k})\Big)$
					</div>
					with $(\forall k \in \{1,\ldots, K\}), x_k \in [0, +\infty[^N$, $\widetilde{x} \in [0, +\infty[^N$, and
					$f:[0, +\infty[^N \mapsto \mathbb{R}^{N}$ bijective, with inverse $f^{-1}$. <br /><br />
					ü§î How to learn the optimal averaging rule/weights?
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
					<aside class="notes">
						Let's consider here the use of averages to ensemble the predictions x k coming from the K weak learners. <br /><br />
						It is possible to choose among different types of averages. For example, using the arithmetic mean is adequate when facing a gaussian noise in the input data, however a geometric mean is better at indicating the central tendency of the averaged values while the harmonic mean is used to compute the average of ratios and rates.<br /><br />
						All of those averages can be written under this form, called a generalised average. This generalised average will be the concept upon which we will build our proposed model ‚û°Ô∏è that we will present in the next section.
					</aside>
				</section>
				<section data-transition="fade-in slide-out" class="section_slide">
					<h1>Part I - Ensemble learning</h1>
					<hr />
					<div style="width: 100%;">
						<div style="float:left; width: 50%;">
							<h2 style="opacity: 0;">-</h2>
							<ol>
								<span class="semi-fade">
									<li>
										Introduction
										<ol>
											<li>Wisdom of the crowd</li>
											<li>Ensemble learning</li>
											<li>Generalised mean</li>
										</ol>
									</li>
								</span>
								<li>
									Proposed model
									<ol>
										<li>$f$-average</li>
										<li>Aggregated $f$-averages</li>
										<li>Stacked aggregated $f$-averages</li>
										<li>Application example</li>
									</ol>
								</li>
								<span class="semi-fade">
									<li>
										Application to Few-Shot Class Incremental Learning
										<ol>
											<li>Motivation & problem setting</li>
											<li>Proposed approach</li>
											<li>Comparative results</li>
										</ol>
									</li>
								</span>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Proposed model: $f$-average</h1>
					<hr />
					<div class="theorem">
						<div class="theorem_title">Generalised average <span id="ref">[Kolmogorov, 1930]</span></div>
						$\widetilde{x} = f^{-1}\Big(\sum_{k=1}^K \omega_{k} f(x_{k})\Big)$
					</div>
					<div>
						with $(\forall k \in \{1,\ldots, K\}), x_k \in [0, +\infty[^N$, $\widetilde{x} \in [0, +\infty[^N$, and
						$f:[0, +\infty[^N \mapsto \mathbb{R}^{N}$ bijective, with inverse $f^{-1}$.
					</div>
					<div style="margin-top: 5%;" class="theorem">
						<div class="theorem_title">Further generalisation: $f$-average</div>
						$\widetilde{x} = f^{-1}\Big(W \boldsymbol{f}(\boldsymbol{x})\Big)$
					</div>
					<table class="notations_table" style="margin-top: 3%;">
						<tr>
							<td>where</td>
							<td>$\boldsymbol{x}$:</td>
							<td>$\in \mathbb{R}^{KN}$, vertical concatenation of the inputs $(x_k)_{1 \le k \le K}$</td>
						</tr>
						<tr>
							<td></td>
							<td>$\boldsymbol{f}$:</td>
							<td>function generalising $f$ operating componentwise from $[0, +\infty[^{KN}$ to $\mathbb{R}^{KN}$</td>
						</tr>
						<tr>
							<td></td>
							<td>$W$:</td>
							<td>$\in [0, + \infty[^{N\times KN}$ such that $W1\kern-0.25em\text{l}_{KN} = 1\kern-0.25em\text{l}_{N}$</td>
						</tr>
					</table>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
					<aside class="notes">
						So as mentioned previously, going from this generalised average, we propose to extend it to the case where the input x is not a vector anymore but rather a concatenation of vectors. In the context of ensemble learning, it would be the concatenation of the K predictions vectors x k.<br /><br />
						This is made possible by concatenating vectors x k into a single vector of size K times N, where K is the number of models and N the prediction dimension,<br /><br />
						by generalising the activation function f by applying it componentwise to each component x k<br /><br />
						and by having a weight matrix W such that its line sum to 1. Finally, the inverse function remains the same.
					</aside>
				</section>
				<section>
					<section class="section_slide">
						<h1>Proposed model: NN modelling of an $f$-average</h1>
						<hr />
						<div class="centred_img">
							<img src="res/f-average.png" style="width:55%" />
							<div><b>Figure</b>: Structure of a neural network modelling an $f$-average</div>
						</div>
						<div style="margin-top: 2%;">An optimal ensembling rule based on the average encoded in $(f,f^{-1})$ can
							be obtained through supervised learning of matrix <span id="celadon">$W$</span>.
						</div>
						<table class="centred_table" style="margin-top: 10%;">
							<thead>
								<tr>
									<th>Mean</th>
									<th>$f(x)$</th>
									<th>$f$ domain</th>
									<th>$f^{-1}(x)$</th>
									<th>$f^{-1}$ domain</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Arithmetic</td>
									<td>$Id$</td>
									<td>$[0,+\infty[^N$</td>
									<td>$Id$</td>
									<td>$[0,+\infty[^N$</td>
								</tr>
								<tr>
									<td>Geometric</td>
									<td>$\big(\ln(\xi_{n}+\epsilon) \big)_{1\le n \le N}$</td>
									<td>$[0,+\infty[^N$</td>
									<td>$\big(\exp(\xi_{n})-\epsilon\big)_{1\le n\le N}$</td>
									<td>$[\ln(\epsilon),+\infty[^N$</td>
								</tr>
								<tr>
									<td>Harmonic</td>
									<td>$\big(h_{\epsilon}(\xi_{n})\big)_{1\le n \le N}$</td>
									<td>$[0,+\infty[^N$</td>
									<td>$\big(h_{\epsilon}(\xi_{n})\big)_{1\le n \le N}$</td>
									<td>$]-\infty,\epsilon^{-1} - \epsilon]^N$</td>
								</tr>
								<tr>
									<td>Quadratic</td>
									<td>$x^2$</td>
									<td>$[0,+\infty[^N$</td>
									<td>$\sqrt{x}$</td>
									<td>$[0,+\infty[^N$</td>
								</tr>
							</tbody>
						</table>
						<div class="centred_table_title">
							<b>Table</b>: Examples for $f$ and $f^{-1}$, and definition domains.
							For geometric and harmonic means, classical mean formulas are retrieved when $\epsilon \to 0$.
						</div>
						<div class="footer">
							<hr />
							<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
						</div>
						<aside class="notes">
							We proposed to model the $f$-average function described previously with a single layer neural network. By choosing adequately f and its inverse function, and by having a matrix W with the constraint previously mentioned, it is possible to model a specific type of average.<br /><br />
							Then with a dataset and a standard training, using a task-relative loss, we learn the optimal weight W.<br /><br />
							We provide in this table activation functions that allow to retrieve standard means formulas. For example, the most straightforward would be the arithmetic mean where both f and its inverse are linear activations.<br /><br />

							So now, we can model a the type of average we want to with a neural network model. However, this does not eliminate us the task of choosing the appropriate average.
						</aside>
					</section>
					<section class="section_slide">
						<h1>Proposed model: NN modelling of an $f$-average</h1>
						<hr />
						<div style="margin-top: 10%;">
							\[
							(\forall \xi \in \mathbb{R})\quad
								h_{\epsilon}(\xi) = \begin{cases}
									\displaystyle-\frac{\xi}{\epsilon^2} +\frac{1}{\epsilon}-\epsilon  & \text{if} \  \xi < 0                         \\~\\
									\displaystyle \frac{1}{\xi+\epsilon}-\epsilon                      & \text{if} \  \xi \in [0,1/\epsilon-\epsilon] \\~\\
									\displaystyle-\epsilon^2 \Big(\xi-\frac{1}{\epsilon}+\epsilon\Big) & \text{if} \  \xi > 1/\epsilon-\epsilon.
								\end{cases}
							\]
						</div>
						<div class="footer">
							<hr />
							<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
						</div>
					</section>
				</section>
				<section class="section_slide">
					<h1>Proposed model: aggregated $f$-averages (AFA)</h1>
					<hr />
					<div class="centred_img">
						<img src="res/aggregated-f-average.png" style="width:40%" />
						<div><b>Figure</b>: Structure of a neural network aggregating $J$ $f$-averages</div>
					</div>
					<table class="notations_table" style="margin-top: 3%;">
						<tr>
							<td>$\forall j \in \{1,\cdots,J\}$,</td>
							<td>$\boldsymbol{x_j}$:</td>
							<td>$\in \mathbb{R}^{KN}$, vertical concatenation of the inputs $(x_k)_{1 \le k \le K}$</td>
						</tr>
						<tr>
							<td></td>
							<td>$\boldsymbol{f_j}$:</td>
							<td>activation function operating componentwise from $[0, +\infty[^{KN}$ to $\mathbb{R}^{KN}$</td>
						</tr>
						<tr>
							<td></td>
							<td><span id="celadon">$W_j$</span>:</td>
							<td>$\in [0, + \infty[^{N\times KN}$ such that $W_j 1\kern-0.25em\text{l}_{KN} = 1\kern-0.25em\text{l}_{N}$</td>
						</tr>
						<tr>
							<td></td>
							<td>$f^{-1}_j$:</td>
							<td>inverse activation function operating componentwise from $\mathbb{R}^{N}$ to $[0, +\infty[^{N}$</td>
						</tr>
						<tr>
							<td></td>
							<td><span id="celadon">$A$</span>:</td>
							<td>$\in \mathbb{R}^{N \times NJ}$</td>
						</tr>
					</table>
					<!-- <div style="margin-top: 2%;">
						‚û°Ô∏è This allows to learn the optimal balance between $J$ different average rules, through the
						learning of $(W_j)_{1\leq j \leq J}$ and $A$.
					</div> -->
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						That is why we propose a larger model that we will call aggregated-f-averages which can model as many types of averages as we would need. For example, J branches implement J times the neural network structure previously presented in order to model J different types of averages. Then, those are aggregated with a final layer A which, once trained, will select and combine in an optimal the result from each different type of average.<br /><br />
						Finally, we set an activation function according to the task, for example by using a softmax for classification.
					</aside>
				</section>
				<section class="section_slide">
					<h1>Proposed model: stacked aggregated $f$-averages</h1>
					<hr />
					<div class="centred_img">
						<img src="res/stacked-aggregated-f-average.png" style="width:85%" />
						<div><b>Figure</b>: Composition of $M$ levels of aggregated $f$-averages. The operation depicted
							with the sign $\|$ represents a concatenation.
						</div>
					</div>
					<table class="notations_table" style="margin-top: 4%;">
						<tr>
							<td>$\forall j \in \{1,\cdots,J\}$ and</td>
							<td>$\boldsymbol{x_{m,j}}$:</td>
							<td>$\in \mathbb{R}^{KN}$, vertical concatenation of the inputs $(x_k)_{1 \le k \le K}$</td>
						</tr>
						<tr>
							<td>$\forall m \in \{1,\cdots,M\}$,</td>
							<td>$\boldsymbol{f_{m,j}}$:</td>
							<td>activation function operating componentwise from $[0, +\infty[^{KN}$ to $\mathbb{R}^{KN}$</td>
						</tr>
						<tr>
							<td></td>
							<td><span id="celadon">$W_{m,j}$</span>:</td>
							<td>$\in [0, + \infty[^{N\times KN}$ such that $W1\kern-0.25em\text{l}_{KN} =
								1\kern-0.25em\text{l}_{N}$
							</td>
						</tr>
						<tr>
							<td></td>
							<td>$f^{-1}_{m,j}$:</td>
							<td>inverse activation function operating componentwise from $\mathbb{R}^{N}$ to $[0,
								+\infty[^{N}$
							</td>
						</tr>
						<tr>
							<td></td>
							<td><span id="celadon">$A$</span>:</td>
							<td>$\in \mathbb{R}^{N \times NJ}$</td>
						</tr>
					</table>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Since we are using neural networks, it is only natural to build a deeper model in the hopes of obtaining better performance. Here we do so by stacking multiple aggregated f averages models.<br /><br />
						Inbetween layers of aggregated f-averages model, a concatenation operation is done and similarly as before, we retrieve again the aggregation layer A at the end of the model.
					</aside>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Application example: simulated noisy classifiers</h1>
					<hr />
					<table class="centred_table" id="afa_toy" style="margin-top: 3%;">
						<tr style="border-bottom: 1px solid #254456;">
							<td></td>
							<td>Accuracy</td>
							<td>$F_1$</td>
						</tr>
						<tr>
							<td>Classifier 1 ($\sigma$ = 0.3)</td>
							<td>58.08</td>
							<td>57.92</td>
						</tr>
						<tr>
							<td>Classifier 2 ($\sigma$ = 0.4)</td>
							<td>49.17</td>
							<td>48.68</td>
						</tr>
						<tr>
							<td>Classifier 3 ($\sigma$ = 0.5)</td>
							<td>44.75</td>
							<td>44.67</td>
						</tr>
						<tr style="border-bottom: 1px solid #254456;">
							<td>Classifier 4 ($\sigma$ = 0.6)</td>
							<td>44.42</td>
							<td>43.94</td>
						</tr>
						<tr>
							<td>Arithmetic mean</td>
							<td>70.42</td>
							<td>70.35</td>
						</tr>
						<tr>
							<td>Geometric mean</td>
							<td>70.08</td>
							<td>70.04</td>
						</tr>
						<tr>
							<td>Harmonic mean</td>
							<td>68.08</td>
							<td>68.04</td>
						</tr>
						<tr style="border-bottom: 1px solid #254456;">
							<td>Majority vote</td>
							<td>62.33</td>
							<td>61.58</td>
						</tr>
						<tr>
							<td>Shallow NN</td>
							<td>70.75</td>
							<td>70.71</td>
						</tr>
						<tr>
							<td>Deep NN</td>
							<td>65.42</td>
							<td>65.41</td>
						</tr>
						<tr style="border-bottom: 1px solid #254456;">
							<td>Weighted avg. NN</td>
							<td>70.42</td>
							<td>70.35</td>
						</tr>
						<tr>
							<td>AFA ($M$ = 1)</td>
							<td>70.83</td>
							<td>70.78</td>
						</tr>
						<tr>
							<td>AFA ($M$ = 2)</td>
							<td>71.08</td>
							<td>70.99</td>
						</tr>
						<tr>
							<td>AFA ($M$ = 3)</td>
							<td><b>72.67</b></td>
							<td><b>72.62</b></td>
						</tr>
						<tr>
							<td>AFA ($M$ = 4)</td>
							<td>71.08</td>
							<td>70.83</td>
						</tr>
					</table>
					<div class="centred_table_title"><b>Table</b>: Performance comparison between standard ensemble learning methods</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						In this first example, we devise a simple scenario to test our proposed ensemble learning method. We simulate four classifiers by generating predictions on which different levels of noise is applied in order to randomise their predictions.<br /><br />
						
						We'll compare our proposed approach with the most common way to perform output fusion which are using an average or a majority vote scheme or a stacked neural network model: a shallow model, which has approximately the same number as parameter as our method, a deeper one which has a similar number of layers but hence a much higher number of parameters and a simple weighted average implemented as a neural network.<br /><br />

						In this example, the method yielding the best performance is our f-average model with a depth of 3. We can see that the f-average with a depth of 4 performs slightly worse. This is probably due to the larger number of parameters. This is corroborated when looking at the other neural network model where we can see that the deep NN performance significantly drops in comparison with the shallow model which performance are close but still lower than our f-average with a depth of 1.
					</aside>
				</section>
				<section data-transition="fade-in slide-out" class="section_slide">
					<h1>Part I - Ensemble learning</h1>
					<hr />
					<div style="width: 100%;">
						<div style="float:left; width: 50%;">
							<h2 style="opacity: 0;">-</h2>
							<ol>
								<span class="semi-fade">
									<li>
										Introduction
										<ol>
											<li>Wisdom of the crowd</li>
											<li>Ensemble learning</li>
											<li>Generalised mean</li>
										</ol>
									</li>
									<li>
										Proposed model
										<ol>
											<li>$f$-average</li>
											<li>Aggregated $f$-averages</li>
											<li>Stacked aggregated $f$-averages</li>
											<li>Application example</li>
										</ol>
									</li>
								</span>
								<li>
									Application to Few-Shot Class Incremental Learning
									<ol>
										<li>Motivation & problem setting</li>
										<li>Proposed approach</li>
										<li>Comparative results</li>
									</ol>
								</li>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Let's move onto the third and last section where we will apply the model we just presented on the few-shot class incremental learning problem.
					</aside>
				</section>
				<section class="section_slide">
					<h1>FSCIL: motivation</h1>
					<hr />
					<div class="centred_img">
						<img src="res/smartalbum.png" style="width: 45%" />
						<div>FSCIL use case: AI-powered smart album.</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Few-shot class incremental learning is a popular problem setting in the field of image recognition. It would be a practical mean to implement a smart photo album which could automatically gather all pictures of a specific subject specified by the user (for example a person, a dog, or a place) into an album. 
					</aside>
				</section>
				<section class="section_slide">
					<h1>FSCIL: problem setting</h1>
					<hr />
					<table class="centred_table">
						<thead>
							<tr>
								<th>Dataset</th>
								<th>$K$</th>
								<th>$n_\text{class\_base}$</th>
								<th>base size</th>
								<th>$n_\text{way}$</th>
								<th>$n_\text{shots}$</th>
								<th>$n_\text{class}$</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>mini-ImageNet</td>
								<td>9</td>
								<td>60</td>
								<td>420</td>
								<td>5</td>
								<td>5</td>
								<td>100</td>
							</tr>
							<tr>
								<td>FGVC-Aircraft</td>
								<td>10</td>
								<td>50</td>
								<td>70</td>
								<td>5</td>
								<td>5</td>
								<td>100</td>
							</tr>
							<tr>
								<td>tiered-ImageNet</td>
								<td>10</td>
								<td>100</td>
								<td>190</td>
								<td>10</td>
								<td>5</td>
								<td>200</td>
							</tr>
							<tr>
								<td>CUB-200</td>
								<td>10</td>
								<td>100</td>
								<td>21</td>
								<td>10</td>
								<td>5</td>
								<td>200</td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title"><b>Table</b>: FSCIL setting values for each dataset</div>
					<table class="centred_table" style="margin-top: 7%;">
						<thead>
							<tr>
								<td><b>Session</b></td>
								<td><b>Training classes</b></td>
								<td><b>Test classes</b></td>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>0</td>
								<td>{0, ‚Ä¶ , 59}</td>
								<td>{0, ‚Ä¶ , 59}</td>
							</tr>
							<tr>
								<td>1</td>
								<td>{60, ‚Ä¶ , 64}</td>
								<td>{0, ‚Ä¶ , 64}</td>
							</tr>
							<tr>
								<td>$\vdots$</td>
								<td>$\vdots$</td>
								<td>$\vdots$</td>
							</tr>
							<tr>
								<td>9</td>
								<td>{95, ‚Ä¶ , 99}</td>
								<td>{0, ‚Ä¶ , 99}</td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title"><b>Table</b>: Example of a setting configuration (mini-ImageNet)</div>
					<div style="margin-top: 7%;"><b>Goal üéØ</b>: learn new classes, using only a few training samples and
						without forgetting previous classes.
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						So, how is this problem set up ? The problem is divided into K successive sessions. In the first session, also called base session, a standard classification dataset with a substantial amount of data is available. Then, for subsequent sessions, some additional classes (a number of n_way classes actually) must be learnt to be recognised by our model in addition of the classes previously learnt. An additional complexity is that for those new classes, only a few number n_shots of training samples are given. And thus until a total number of n_class is reached.<br /><br />
						
						For example, let's take the mini-ImageNet dataset setting. We have in base session 60 classes to be learnt (with a standard amount of training data) and then, for the next session, we have training data for 5 new classes. At the end of session 1, those 5 new classes must be recognisable by the model along with the 60 previous classes. And so on, until last session (the session number 9) where again 5 new classes are given bringing the total to a 100 classes.<br /><br />

						Long story short, the goal is to learn to classify new classes, using only as few training samples and without forgetting previous classes.<br /><br />

						Most of the state of the art methods use base session to train the feature extractor part of their model (thanks to the substantial amount of data). Then in subsequent sessions, the feature extractor weights are frozen and the classifier head of their model grows in size and is trained with new data.<br /><br />

						In our case, we propose to use ensemble learning and, to our knowledge, we are the first to do so.
					</aside>
				</section>
				<section class="section_slide">
					<h1>FSCIL: proposed approach - pipeline</h1>
					<hr />
					<div class="centred_img">
						<img src="res/fscil_afa_pipeline.png" style="width: 45%" />
						<div style="width: 70%;"><b>Figure</b>: scheme of our proposed approach. Each session is considered
							as a few-shot task in order to build a set of models that are then ensembled using our AFA
							method.
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
					<aside class="notes">
						So our proposed method is to consider few-shot class incremental learning as a succession of few-shot tasks.<br /><br />
						
						Similarly as other methods from the state of the art, we reuse the feature extractor trained during base session.<br /><br />
						Then at each session, we train a weak classifier with a few-shot method with the data from each of those session. The weak classifier will then be specialised on its own set of classes from its own session.<br /><br />
						
						However, it will not be able to output prediction for classes from other sessions. That's why we ensemble all weak classifiers prediction with our proposed f-average model. Once trained, it will be able, given the predictions from all weak classifiers, to output the most likely class prediction, which could be any class from any session.<br /><br />

						Let's get into more details and see how we train each weak classifier.
					</aside>
				</section>
				<section class="section_slide">
					<h1>FSCIL: proposed approach - weak learners training</h1>
					<hr />
					<h2>$H_{W_k}$: nearest-neighbor classifier <span id="ref">[Wang, 2019]</span></h2>
					<div class="theorem">
						<div class="theorem_title">Mean centroid of each training class</div>
						\[
							(\forall k \in \{1, \dots, K\}) (\forall c \in C_k^\text{train}) \quad \bar{z_c} = \frac{1}{n_{\text{shots}}}\sum_{n = 1}^{n_{\text{shots}}} z_{n,c}
						\]
					</div>
					where $z$ is the $\ell_2$-normalised feature vector of a sample image $i$
					\[
						z = \frac{f_\theta(i)}{\|f_\theta(i)\|_2}
					\]
					<div class="theorem" style="margin-top: 2%; padding-bottom: 0.1em">
						<div class="theorem_title">Estimated class $\widehat{c}(i)$ of a test image $i$</div>
						\[
							\widehat{c}(i) = \argmin_{c \in C_k^\text{test}} \|z - \bar{z_c}\|^2
						\]
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
					<aside class="notes">
						We use a popular yet simple few-shot learning method: a nearest-neighbor classifier. For each new class, a mean centroid is computed using the training data available in the feature space.<br /><br />
						Then, for each new test image, its projection in the feature is compared against all centroids of all classes and its class is predicted to be the same as the closest centroid, in the feature space. 
					</aside>
				</section>
				<section>
					<section class="section_slide">
						<h1>FSCIL: proposed approach - padding</h1>
						<hr />
						<div style="width: 100%; margin-top: 3%;">
							<div style="float:left; width: 60%; border-right: 1px solid #254456;">
								<div class="centred_img">
									<img src="res/fscil_afa_pipeline.png" style="width: 85%" />
								</div>
							</div>
							<div style="float:right; width: 39%;">
								<div style="margin-top: 40%;">
									<b>Padding</b>: <br />
									<ul>
										<li> if the prediction confidence is higher than a threshold then attribute a greater weight to $x_{k,n}$
										</li>
										<li> else greater weight to $p_{k,n}$
										</li>
									</ul>
								</div>
							</div>
						</div>
						<div class="footer">
							<hr />
							<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
							</div>
						</div>
						<aside class="notes">
							As I said earlier, predictions from weak classifiers are gathered and used as input for our ensembling model. However, given the fact that the number of class grows at each session, the prediction vectors are not of the same dimension. To solve this issue, we implement a padding method to level the dimensions. To further refine the padding, we compute it in function of the level of confidence of the weak classifier prediction given by max (x). If this value is above a certain threshold (set as a hyperparameter), then the prediction is considered as likely to be within the set of classes learnt by the weak classifier, else, it is more likely part of the other classes, filled by the padding value. In that case, we give a higher probability to the padding.
						</aside>
					</section>
					<section class="section_slide">
						<h1>FSCIL: proposed approach - padding</h1>
						<hr />
						<div style="margin-top: 15%;"">
							\[\begin{align*}
								x'_i & = \begin{cases}
									\frac{1}{2} (\frac{N_k}{N_K} + 1) x_i, & \qquad \; \text{if} \max\limits_{1 \le i \le N_k}x_i \geq t\\
									\frac{1}{2} (\frac{N_k}{N_K}) x_i, & \qquad \; \text{otherwise}
								\end{cases} \\ ~ \\
								p'_i & = \begin{cases}
									(1 - \frac{1}{2} (\frac{N_k}{N_K} + 1)) p_i, & \text{if} \max\limits_{1 \le i \le N_k} x_i \ge t\\
									(1 - \frac{1}{2} (\frac{N_k}{N_K})) p_i, & \text{otherwise}
								\end{cases}
							\end{align*}\]
						</div>
						<div class="footer">
							<hr />
							<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
							</div>
						</div>				
					</section>
				</section>
				<section data-auto-animate class="section_slide">
					<h1>FSCIL: Comparatives results - ensemble learning</h1>
					<hr data-id="header_hr"/>
					<div style="width: 100%; ">
						<div style="float:left; width: 30%;">
							<ul style="margin-top: 30%;">
								<li class="fragment fade-down"><b>Averages</b>
									<ul>
										<li>Arithmetic</li>
										<li>Geometric</li>
										<li>Harmonic</li>
										<li>Majority voting</li>
									</ul>
								</li>
								<li class="fragment fade-down" style="padding-top: 1.5em"><b>Stacked NN models</b>
									<ul>
										<li>Shallow NN</li>
										<li>Deep NN</li>
										<li>Weighted average NN</li>
										<li>AFA</li>
									</ul>
								</li>
							</ul>
						</div>
						<div class="fragment fade-in" style="float:right; width: 69%; border-left: 1px solid #254456;">
							<div class="centred_img">
								<img data-id="perf_graph" src="res/fscil_ensemble_perf.png" style="width: 65%" />
								<div style="width: 70%;"><b></b>
								</div>
							</div>
						</div>
					</div>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
					<aside class="notes">
						Cite datasets and colours<br /><br />

						F1 score the higher the better<br /><br />

						We can see that at the last session our model performs better than every other methods. Overall, we can observe that other methods are not consistent accross sessions and datasets, with some being better than others in specific cases but not always. In particular we can observe that depsite relatively good performance on the CUB-200 dataset, the deep NN is very inconstistent in learning the later sessions. That is certainly due to its higher number of parameters that are not trained correctly due to a low data regime imposed by the problem setting. Let us remark that the constraints that we impose on the architecture of our model do not hinder its performance but rather enhance it by improving its training stability.
					</aside>
				</section>
				<section data-auto-animate class="section_slide">
					<h1>FSCIL: Comparatives results - state of the art</h1>
					<hr data-id="header_hr"/>
					<div class="centred_img">
						<img data-id="perf_graph" src="res/fscil_ensemble_perf_sota.png" style="width: 46%" />
						<div style="width: 70%;"><b></b>
						</div>
					</div>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
					<aside class="notes">
						Now, if we compare our method against state of the art methods designed specifically for few shot class incremental learning, we can observe that the performance are actually comparable. Once again F1 score the higher the better. The best method called FACT does outperform us in most datasets us but our method comes first on mini-ImageNet and second or third on other datasets as this graph shows.
					</aside>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Part I: Conclusion</h1>
					<hr />
					<ul>
						<li style="margin-top: 5%;">General framework for describing averaging rules in ensemble learning</li>
							<ul>
								<li>Focus on output fusion</li>
							</ul>
						<li style="padding-top: 2em;">Optimal ensembling through training of an interpretable neural network architecture
							<ul>
								<li>$f$-average: neural network modelling an average</li>
								<li>Aggregating multiple $f$-averages</li>
								<li>Stacking multiple aggregated $f$-averages</li>
							</ul>
						</li>
						<li style="padding-top: 2em;">Application to few-shot class incremental learning
							<ul>
								<li>An illustrative scenario to showcase our ensemble learning model</li>
								<li>Best performance among ensemble learning methods</li>
								<li>Comparable performance with specialised state-of-the-art methods</li>
							</ul>
						</li>
					</ul>
					<div style="margin-top: 5%; border-left: 5px solid #254456; padding-left: 1em;">
						M. Vu, √â. Chouzenoux, I. Ben Ayed, J.-C. Pesquet, Aggregated $f$-average Neural Network applied to Few-Shot Class Incremental Learning, <em>submitted to Signal Processing</em>, 2024.
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
					<aside class="notes">
						In conclusion of this first part on ensemble learning, we presented a method inspired by averaging rules to perform the output fusion step of an ensembling method.<br /><br />
						‚û°Ô∏è We proposed an architecture which can model and aggregate multiple types of averages by choosing adequates activation functions and by imposing some constraints on weight matrices.<br /><br />
						‚û°Ô∏è Finally, we used the few shot class incremental learning setting as it is a problem well suited to illustrate the output fusion method that we propose. Indeed, the problem naturally sets up an ensemble of diverse weak classifiers, letting us showcase the strength of our output fusion models.<br /><br />
						
						In terms of performance, we show that our method outperforms standard ensembling methods such as averages and stacked models partly thanks to an increased training stability and we also show that altough not the best, our method performance are comparable with state of the art methods designed specifically for the problem of few shot class incremental learning.
					</aside>
				</section>
				<section data-transition="fade-in slide-out" class="section_slide">
					<h1>Part II - Deep learning for photorefraction</h1>
					<hr />
					<div style="width: 100%;">
						<div style="float:left; width: 50%;">
							<ol>
								<li>
									Introduction
									<ol>
										<li>Context</li>
										<li>Objectives</li>
										<li>Eye refractive error</li>
									</ol>
								</li>
								<span class="semi-fade">
									<li>
										Proposed pipeline
										<ol>
											<li>Photorefraction principle</li>
											<li>Essilor capture device</li>
											<li>Datasets, CNN architecture & training</li>
											<li>End-to-end pipeline</li>
										</ol>
									</li>
									<li>
										Implementation & performance
										<ol>
											<li>Regression models benchmark</li>
											<li>Real versus synthetic data</li>
											<li>Performance comparison with other device</li>
											<li>Implementation performance</li>
											<li>Ensembling regression models</li>
										</ol>
									</li>
									<li>
										Robustness analysis
										<ol>
											<li>Numerical evaluation of robustness</li>
											<li>Improve robustness with a denoiser</li>
											<li>Controlled Lipschitz constant training</li>
										</ol>
									</li>
								</span>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Let's get into the second part of this presentation where we will be talking about the project that we've done in collaboration with Essilor about a deep learning-based photorefraction method. As previously, let us start with a contextual introduction and some introduction about the basic functioning of an eye.
					</aside>
				</section>
				<section data-auto-animate class="section_slide">
					<h1>Introduction: context</h1>
					<hr data-id="header_hr"/>
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/blindness_vietnam.jpg" style="width:70%" />
					</div>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						‚û°Ô∏è
					</aside>
				</section>
				<section data-auto-animate class="section_slide">
					<h1>Introduction: context</h1>
					<hr data-id="header_hr"/>
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/blindness_vietnam.jpg" style="width:50%" />
					</div>
					<div style="margin-top: 5%;">
						<ul>
							<li class="fragment fade-down" style="padding-top: .5em;">1 billion people with vision impairment unadressed</li>
							<li class="fragment fade-down" style="padding-top: .5em;">uncorrected refractive errors: a leading cause of vision impairment</li>
						</ul>
					</div>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						‚û°Ô∏è In the world, an estimated number of 1 billion people suffer from vision impairment without being addressed. Actually, everyone will experience at least once in their lifetime some eye condition, for those who are fortunate to live long enough. Vision impairment can occur at any age and have serious implications in everyday life.<br /><br />
						
						‚û°Ô∏èIt can be caused by many things but the leading causes are uncorrected refractive errors and cataracts. In this project, we will focus only on uncorrected refractive errors such as hyperopia or myopia and astigmatism.
					</aside>
				</section>
				<section class="section_slide">
					<h1>Introduction: context</h1>
					<hr />
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/myopia_projection.png" style="width: 70%" />
						<div><b>Source</b>: Global prevalence of myopia and high myopia and temporal trends from 2000 through 2050 <span id="ref">[Holden, 2016]</span></div>
					</div>
					<div style="margin-top: 4%;">
						<ul>
							<li class="fragment fade-down">Myopia might affect almost 50 % of the population by 2050</li>
							<li class="fragment fade-down" style="padding-top: 1em;">A critical challenge for developing countries</li>
						</ul>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						For example, ‚û°Ô∏è myopia is believed to affect 50% of the population by 2050. As you can see on this graph, Asia will be the most impacted. In particular, ‚û°Ô∏è developing countries face a critical challenge. In one hand, the lack of prevention in those countries only worsens the situation and in the other hand a deficient healthcare system can prevent an efficient screening of the population. It has been shown that the education of young people can be hindered by vision impairment  and more generally it reduces quality of life which can already be difficult in developing countries.<br /><br />
						
						There exists cost-effective solutions to vision loss but they of course cannot be applied if the condition is not first correctly diagnosed. That is why, nowadays, accessible and affordable screening tools are crucial to abolish vision impairment around the world.
					</aside>
				</section>
				<section class="section_slide">
					<h1>Introduction: project objectives</h1>
					<hr data-id="header_hr"/>
					<div style="margin-top: 13%;">
						Design a screening tool <br />
						<ul>
							<li style="padding-top: 1em;">accessible (easy, comfortable to use even for non-experts)</li>
							<li style="padding-top: 1em;">affordable</li>
							<li style="padding-top: 1em;">with competitive performance</li>
							<li style="padding-top: 1em;">reliable</li>
						</ul>
					</div>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						In the lights of this challenges, we, in collaboration with Essilor, aim at designing and developing a screening tool which needs to be<br /><br />
						
						widely accessible (which means easy and comfortable to use even for non-expert people)<br /><br />
						and affordable (keeping in mind the developing countries we mentioned earlier) but without sacrificing its screening performance.<br /><br />

						And finally, given that it is a medical device, we will also need to demonstrate its reliability and robustness.<br /><br />
						
						At the end of this part, we will come back to these requirements to see if and how we addressed them. In the next section, I will present you the pipeline that we propose in order to answer to the problematic that was raised previously.<br /><br />
						
						But first, we‚Äôll have a quick introduction on eye refractive errors. 
					</aside>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Introduction: eye refractive error</h1>
					<hr />
					<div class="centred_img">
						<img src="res/zb/eye_anatomy.png" style="width: 40%" />
					</div>
					<table style="margin-top: 2%;">
						<tr>
							<td style="text-align: center;">Sphere</td>
							<td style="text-align: center;">Cylinder</td>
							<td style="text-align: center;">Axis</td>
						</tr>
						<tr>
							<td style="text-align: center;">$[-13, 10]$ (dioptre)</td>
							<td style="text-align: center;">$[-6, 0]$ (dioptre)</td>
							<td style="text-align: center;">$[0, 180]$ (degree)</td>
						</tr>
					</table>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Here is the anatomy of an eye. When looking at an object, light rays come through the pupil and the cornea located here. They go through here and create an image on retina at the bottom of the eye. Finally, the optic nerve transmits the information to the brain. A refractive error happens when the shape of the eye is not perfect. For example, if the eye is too big or too small, the image will not be created on the retina but before or after it, creating a blurry image to the brain. This is what we call myopia and hyperopia. Another problem also happens when the cornea is not perfectly round but looks more like a rugby ball. This refracts ununiformly the light rays that are coming in and will in the end distort the image.<br /><br />
						
						Sphere, Cylinder and Axis are parameters describing those conditions and they are actually what will appear on your glasses prescriptions after you‚Äôve visited an ophthalmologist or an optometrist. In theory their range are unlimited but the vast majority of the population are within these ranges.
					</aside>
				</section>
				<section data-transition="fade-in slide-out" class="section_slide">
					<h1>Part II - Deep learning for photorefraction</h1>
					<hr />
					<div style="width: 100%;">
						<div style="float:left; width: 50%;">
							<ol>
								<span class="semi-fade">
									<li>
										Introduction
										<ol>
											<li>Context</li>
											<li>Objectives</li>
											<li>Eye refractive error</li>
										</ol>
									</li>
								</span>
								<li>
									Proposed pipeline
									<ol>
										<li>Photorefraction principle</li>
										<li>Essilor capture device</li>
										<li>Datasets, CNN architecture & training</li>
										<li>End-to-end pipeline</li>
									</ol>
								</li>
								<span class="semi-fade">
									<li>
										Implementation & performance
										<ol>
											<li>Regression models benchmark</li>
											<li>Real versus synthetic data</li>
											<li>Performance comparison with other device</li>
											<li>Implementation performance</li>
											<li>Ensembling regression models</li>
										</ol>
									</li>
									<li>
										Robustness analysis
										<ol>
											<li>Numerical evaluation of robustness</li>
											<li>Improve robustness with a denoiser</li>
											<li>Controlled Lipschitz constant training</li>
										</ol>
									</li>
								</span>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						So let's get into the details of our pipeline with first some explanations around the principle of photorefraction which is, as said previously, at the core of our pipeline. This method was chosen along the device design by the Essilor team in order to answer some of the constraints mentioned previously.
					</aside>
				</section>
				<section class="section_slide">
					<h1>Proposed pipeline: photorefraction principle</h1>
					<hr />
					<div class="centred_img">
						<img src="res/zb/photorefraction_scheme.png" style="width: 50%" />
					</div>
					<table class="notations_table" style="margin: 4% auto;">
						<tr>
							<td></td>
							<td>$O$:</td>
							<td>center of the camera aperture</td>
							<td style="padding-left: 7em;">$e$:</td>
							<td>eccentricity</td>
						</tr>
						<tr>
							<td></td>
							<td>$r_c$:</td>
							<td>radius of the camera aperture</td>
							<td>$AB$:</td>
							<td>spot on the retina</td>
						</tr>
						<tr>
							<td></td>
							<td>$C$:</td>
							<td>camera aperture extremity</td>
							<td>$r$:</td>
							<td>pupil radius</td>
						</tr>
						<tr>
							<td></td>
							<td>$L$:</td>
							<td>light point source</td>
							<td>$d$:</td>
							<td>camera to eye distance</td>
						</tr>
					</table>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						When some source emits light, it goes through the eye and bounces back on the retina creating a light pattern on the pupil.  It will appear like this when facing the eye. ‚û°Ô∏è
					</aside>
				</section>
				<section class="section_slide">
					<h1>Proposed pipeline: photorefraction principle</h1>
					<hr />
					<div style="width: 100%; margin-top: 3%;">
						<div style="float:left; width: 35%;">
							<div class="centred_img">
								<img src="res/res_deep_photorefraction/eye_scheme.png" style="width: 85%; margin: 25% auto;"/>
							</div>
						</div>
						<div style="float:right; width: 64%; margin-top: 13%; text-align: left;">
							The light pattern whose size $s$, shape and orientation depend on: <br />
							<ul>
								<li style="padding-top: 1em;">the subject's Sphere, Cylinder and Axis parameters,</li>
								<li style="padding-top: 1em;">the light source position wrt. the camera aperture,</li>
								<li style="padding-top: 1em;">the measurement distance $d$.</li>
							</ul>
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
					<aside class="notes">
						If we know the light source position wrt. the camera, the measurement distance d, we can determine the subject‚Äôs sphere cylinder axis parameters by observing the light pattern. <br /><br />
						There already exists some methods to analyse this light pattern to determine the ametropia but we propose to use machine learning to do so.
					</aside>
				</section>
				<section class="section_slide">
					<h1>Proposed pipeline: Essilor photorefraction device</h1>
					<hr />
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/pipeline_archi_1.png" style="width: 35%;"/>
					</div>
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/captured_eye.png" style="width: 90%; margin-top: 3%;"/>
					</div>
					<div style="width: fit-content; margin: 3% auto;">Capture process < 400 ms</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						So let‚Äôs first begin with the photorefraction device of our pipeline.<br /><br />
						It is composed of a camera in the centre, surrounded by 12 LEDs. One at the time, a LED will light up and a picture of the eyes will synchronously be captured by the camera at the same time. The LED emits infrared light in order not to blind the patient to prevent them to close their eyes by reflex.<br /><br />
						 In those pictures, the light pattern we mentioned previously will appear like so. Twelve LEDs are necessary to ensure some redundancy of the information because of blind spots than can appear at some angles and eccentricities.<br /><br />
						 The capture process lasts less than 400 ms minimising perturbations related to the patient movements.<br /><br />
						 This device was developed by Essilor and the latest iteration was improved, in part thanks to some feedbacks we provided.<br /><br />
						 
						 The goal is now to retrieve the parameters Sphere, Cylinder and Axis from those images using deep learning. That is why they also provided with various kinds of datasets, which, as you know, are essential in machine learning problems.
					</aside>
				</section>
				<section class="section_slide">
					<h1>Proposed pipeline: datasets</h1>
					<hr />
					<table id="table_datasets">
						<tr>
							<td><h3>Datasets</h3></td>
							<td></td>
						</tr>
						<tr>
							<td><h4>Synthetic:</h4></td>
							<td><h4>Real:</h4></td>
						</tr>
						<tr>
							<td>+ Free / unlimited</td>
							<td>‚àí Expensive / limited</td>
						</tr>
						<tr>
							<td>+ 1M samples</td>
							<td>‚àí 3000 samples</td>
						</tr>
						<tr>
							<td>‚àí Artificial</td>
							<td>+ Genuine</td>
						</tr>
						<tr>
							<td>+ Selected distribution</td>
							<td>‚àí Uncontrolled distribution</td>
						</tr>
					</table>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						First, they developed a simulator that provides us pairs of images and their corresponding sphere cylinder axis parameters. This dataset can be as big as we want (about 1 million samples) with the desired distribution of parameters. However, its big drawback is that it is artificial and thus does not exactly match real images. For example, it cannot simulate input perturbations such as motion blur.<br /><br />
						
						To alleviate this problem, they also collected a dataset of real images. They organised some acquisition campaign and recruited volunteers and captured images with the device and then evaluated their ametropia in a standard manner. However, obviously, this process is quite expensive and thus is very limited in size. And of course, since it is collected on volunteers we cannot control its parameter distribution.<br /><br />
						
						We will see in the next section how we leverage these two datasets.
					</aside>
				</section>
				<section data-auto-animate class="section_slide">
					<h1>Proposed pipeline: neural network architecture</h1>
					<hr data-id="hr_header"/>
					<table id="target_table" style="margin-top: 2%;">
						<tr>
							<td>Sphere</td>
							<td>Cylinder</td>
							<td>Axis</td>
							<td>‚û°Ô∏è</td>
							<td>$M$</td>
							<td>$J_0$</td>
							<td>$J_{45}$</td>
						</tr>
						<tr>
							<td>$[-13, 10]$ (dioptre)</td>
							<td>$[-6, 0]$ (dioptre)</td>
							<td>$[0, 180]$ (degree)</td>
							<td></td>
							<td>$[-14, 10]$ (dioptre)</td>
							<td>$[-3, 3]$ (dioptre)</td>
							<td style="text-align: center;">$[-2, 3]$ (dioptre)</td>
						</tr>
					</table>
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/pipeline_archi_2.png" style="width: 75%; margin-top: 5%;"/>
					</div>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Now that we have our data, we can train our model. It is a regression problem so we took convolutional neural network which takes as input a tensor of 12 images and predicts ONE parameter.<br /><br />
						I told you earlier about the sphere cylinder and axis parameters but we actually predict three other parameters called M J0 J45 which are their equivalent in a cartesian coordinate system. They are equivalent and can be translated one way or the other with a simple formula but we prefer using them because they are independent of each other. 
					</aside>
				</section>
				<section data-auto-animate class="section_slide">
					<h1>Proposed pipeline: neural network architecture</h1>
					<hr data-id="hr_header"/>
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/pipeline_archi_2.png" style="width: 65%; margin-top: 2%;"/>
					</div>
					<ul class="fragment fade-down" id="cnn_models_list" style="margin-top: 2%;">
						<li>regression: from images to a parameter value (mean squared error loss)</li>
						<li>a different set of weights for each target parameter</li>
						<li>lightweight: running on a embedded software</li>
						<li>fast inference for user experience</li>
						<li>without compromising performance</li>
					</ul>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						As I told you previously, the model predicts one parameter so we train 3 different model, one for each parameter. This type of model was selected thanks to its performance among many other model architectures, its lightweight memory and its fast inference time which are crucial in a smartphone embedded software.
					</aside>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Proposed pipeline: complete pipeline</h1>
					<hr />
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/pipeline_archi.png" style="width: 80%; margin-top: 2%;"/>
					</div>
					<div style="width: 100%; margin-top: 3%;">
						<div style="float:left; width: 50%;">
							A smartphone coordinates the pipeline <br />
							<ul id="cnn_models_list">
								<li>is the user interface through an app</li>
								<li>triggers and operates the capture device</li>
								<li>runs the image processing algorithm</li>
								<li>runs the neural network inference</li>
								<li>determines and displays Sphere, Cylinder and Axis</li>
							</ul>
						</div>
						<div style="float:right; width: 50%;">
							<!-- <div class="centred_img">
								<img src="res/res_deep_photorefraction/device.png" style="width: 50%;"/>
							</div> -->
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Our whole pipeline is coordinated by a smartphone. It serves as the user interface through an app. It triggers and operates the photorefraction device, runs the image pre-processing and the neural network inference and finally determines SCA. This end-to-end pipeline allows us to control every step of the process, allowing us to optimise each component to produce the best possible performance.
					</aside>
				</section>
				<section data-transition="fade-in slide-out" class="section_slide">
					<h1>Part II - Deep learning for photorefraction</h1>
					<hr />
					<div style="width: 100%;">
						<div style="float:left; width: 50%;">
							<ol>
								<span class="semi-fade">
									<li>
										Introduction
										<ol>
											<li>Context</li>
											<li>Objectives</li>
											<li>Eye refractive error</li>
										</ol>
									</li>
									<li>
										Proposed pipeline
										<ol>
											<li>Photorefraction principle</li>
											<li>Essilor capture device</li>
											<li>Datasets, CNN architecture & training</li>
											<li>End-to-end pipeline</li>
										</ol>
									</li>
								</span>
								<li>
									Implementation & performance
									<ol>
										<li>Regression models benchmark</li>
										<li>Real versus synthetic data</li>
										<li>Performance comparison with other device</li>
										<li>Implementation performance</li>
										<li>Ensembling regression models</li>
									</ol>
								</li>
								<span class="semi-fade">
									<li>
										Robustness analysis
										<ol>
											<li>Numerical evaluation of robustness</li>
											<li>Improve robustness with a denoiser</li>
											<li>Controlled Lipschitz constant training</li>
										</ol>
									</li>
								</span>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Now comes the section of our experimental results.
					</aside>
				</section>
				<section class="section_slide">
					<h1>Performance: regression models benchmark</h1>
					<hr />
					<table class="centred_table" id="regression_table" style="margin-top: 2%;">
						<tr>
							<td>Target</td>
							<td>Mandatory</td>
							<td>Good to have</td>
						</tr>
						<tr>
							<td>$M$</td>
							<td>1.00</td>
							<td>0.75</td>
						</tr>
						<tr>
							<td>$J_0$</td>
							<td>0.50</td>
							<td>0.30</td>
						</tr>
						<tr>
							<td>$J_{45}$</td>
							<td>0.50</td>
							<td>0.30</td>
						</tr>
					</table>
					<div class="centred_table_title"><b>Table</b>: industrial performance requirements for the error standard deviation (in dioptre)</div>
					<table class="centred_table" id="regression_table" style="margin-top: 3%;">
						<thead>
							<tr>
								<th></th>
								<th></th>
								<th>$M$</th>
								<th></th>
								<th></th>
								<th>$J_0$</th>
								<th></th>
								<th></th>
								<th>$J_{45}$</th>
								<th></th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>Error</td>
								<td>BSM<sup>1</sup></td>
								<td>FC</td>
								<td>CNN</td>
								<td>BSM<sup>1</sup></td>
								<td>FC</td>
								<td>CNN</td>
								<td>BSM<sup>1</sup></td>
								<td>FC</td>
								<td>CNN</td>
							</tr>
							<tr>
								<td>min</td>
								<td>-4.66</td>
								<td>-2.31</td>
								<td><b>-2.16</b></td>
								<td>-2.82</td>
								<td>-1.22</td>
								<td><b>-0.97</b></td>
								<td>-2.56</td>
								<td><b>-0.83</b></td>
								<td>-1.17</td>
							</tr>
							<tr>
								<td>mean</td>
								<td>0.05</td>
								<td><b>0.02</b></td>
								<td>-0.03</td>
								<td>-0.04</td>
								<td><b>0.01</b></td>
								<td>0.02</td>
								<td>0.01</td>
								<td>0.01</td>
								<td>-0.01</td>
							</tr>
							<tr>
								<td>std</td>
								<td>0.75</td>
								<td>0.60</td>
								<td><b>0.56</b></td>
								<td>0.43</td>
								<td>0.28</td>
								<td><b>0.26</b></td>
								<td>0.32</td>
								<td>0.23</td>
								<td><b>0.22</b></td>
							</tr>
							<tr>
								<td>max</td>
								<td>5.73</td>
								<td>2.86</td>
								<td><b>2.50</b></td>
								<td>2.62</td>
								<td>1.01</td>
								<td><b>0.88</b></td>
								<td>2.36</td>
								<td>1.13</td>
								<td><b>0.80</b></td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title">
						<b>Table</b>: Regression models comparison on the dataset of real images (in dioptre)
					</div>
					<ul id="cnn_models_list" style="margin-top: 3%;">
						<li>Since a biased model can easily be corrected by offsetting predictions, our main evaluation criterion is the error <b>standard deviation</b>.</li>
						<li>To better assess these results, recall that glasses are prescribed with a precision of 0.25 D.</li>
					</ul>
					<div style="margin-top: 2%; font-size: x-large;">1. <span id="ref">[Gekeler, 1997]</span></div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						First, we evaluate the choice of our regression model. As you can see in this table (unit is dioptre), it works better than the bright slope method, an eccentric photorefraction method that does not use machine learning and also works better than a fully connected model.<br /><br />
						To evaluate the results, focus the results on the standard deviation which is our main criterion. So looking at this line, we clearly see that the CNN outperforms the other models.<br /><br />
						Also keep in mind that glasses are prescribed with a precision of 0.25D.
					</aside>
				</section>
				<section class="section_slide">
					<h1>Performance: synthetic vs. real data</h1>
					<hr />
					<table id="synth_real_data_table" style="margin-top: 10%;">
						<thead>
							<tr>
								<th>Train</th>
								<th>Test</th>
								<th>$M$</th>
								<th>$J_0$</th>
								<th>$J_{45}$</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td rowspan="2" style="vertical-align : middle;">Synthetic</td>
								<td>Synthetic</td>
								<td>0.10</td>
								<td>0.07</td>
								<td>0.07</td>
							</tr>
							<tr>
								<td>Real</td>
								<td>0.70</td>
								<td>0.33</td>
								<td>0.32</td>
							</tr>
							<tr>
								<td rowspan="2" style="vertical-align : middle;">Real</td>
								<td>Synthetic</td>
								<td>2.70</td>
								<td>0.58</td>
								<td>0.52</td>
							</tr>
							<tr>
								<td>Real</td>
								<td>0.56</td>
								<td><b>0.26</b></td>
								<td>0.22</td>
							</tr>
							<tr>
								<td rowspan="2" style="vertical-align : middle;">Mix</td>
								<td>Synthetic</td>
								<td>0.20</td>
								<td>0.09</td>
								<td>0.09</td>
							</tr>
							<tr>
								<td>Real</td>
								<td><b>0.54</b></td>
								<td>0.28</td>
								<td><b>0.21</b></td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title">
						<b>Table</b>: CNN prediction error standard deviation wrt. ground truth (in D.) on synthetic and real data.
					</div>
					<ul id="cnn_models_list" style="margin-top: 5%;">
						<li>Synthetic images are not sufficient by themselves</li>
						<li>Training on real images yields better results</li>
						<li>Mixed data training yields best results</li>
						<li>Synthetic images are useful to test on a wider range of parameters</li>
					</ul>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Now, let us see the performance of our model on the different datasets. When trained on synthetic images, the CNN performance has very high performance on synthetic images but is not as good on real images. When training on real images, we of course get better results. We get even better results when using a mix of both synthetic and real images. <br /><br />

						Synthetic images are also useful to test on wider and more complete range of parameters. Indeed, the test set of real images being very limited in size, it is not uniformly distributed across the parameters range.<br /><br />
					</aside>
				</section>
				<section class="section_slide">
					<h1>Performance: mix training model</h1>
					<hr />
					<div class="centred_img" style="margin-top: 7%">
						<img src="res/zf/ba/mixte_M_M_10150SI-M.png" style="width: 33%; display: inline;"/>
						<img src="res/zf/ba/mixte_J0_J0_10150SI-M.png" style="width: 33%; display: inline;"/>
						<img src="res/zf/ba/mixte_J45_J45_10150SI-M.png" style="width: 33%; display: inline;"/>
						<div><b>Figure</b>: Bland-Altman plots for the CNN with mix training</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Performance: comparison with competing device</h1>
					<hr />
					<table id="device_comp_table" style="margin-top: 10%;">
						<thead>
							<tr>
								<th></th>
								<th colspan="2">2WIN<sup>1</sup></th>
								<th colspan="2">2WIN<sup>2</sup></th>
								<th colspan="2">Ours</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td></td>
								<td>mean</td>
								<td>std</td>
								<td>mean</td>
								<td>std</td>
								<td>mean</td>
								<td>std</td>
							</tr>
							<tr>
								<td>$M$</td>
								<td>0.41</td>
								<td>1.37</td>
								<td>1.25</td>
								<td>1.15</td>
								<td><b>-0.03</b></td>
								<td><b>0.56</b></td>
							</tr>
							<tr>
								<td>$J_0$</td>
								<td>-0.08</td>
								<td>0.35</td>
								<td>0.19</td>
								<td>0.45</td>
								<td><b>0.02</b></td>
								<td><b>0.26</b></td>
							</tr>
							<tr>
								<td>$J_{45}$</td>
								<td><b>0.00</b></td>
								<td>0.25</td>
								<td>-0.05</td>
								<td><b>0.15</b></td>
								<td>-0.01</td>
								<td>0.22</td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title">
						<b>Table</b>: CNN prediction error standard deviation wrt. ground truth (in D.) on synthetic and real data.
					</div>
					<ul id="cnn_models_list" style="margin-top: 5%;">
						<li>Best results on $M$ and $J_0$</li>
						<li>Competitive on $J_{45}$</li>
					</ul>
					<div style="margin-top: 7%; font-size: x-large;">
						1. <span id="ref">[Martin, 2020]</span><br />
						2. <span id="ref">[Kurent, 2022]</span><br />
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Last but not least, we compare the combination of our device and deep learning pipeline against a competitor device already commercialised. Let us remark that this device is more expensive than our device. It was tested in different conditions with results reported in two different papers and we seem to produce better results than them, except maybe for J45 for which we still have very similar results.  (Kurent: 120 patients, Martin: 162 patients)
					</aside>
				</section>
				<section class="section_slide">
					<h1>Performance: implementation performance</h1>
					<hr />
					<ul id="cnn_models_list" style="margin-top: 10%;">
						<li>offline training
							<ul>
								<li>Python Tensorflow</li>
								<li>Nvidia RTX A6000 GPU</li>
								<li>~50 hours per model</li>
							</ul>
						</li>
						<li>inference: 33 ms per eye per target (200 ms in total)</li>
						<li>smartphone embedded software: ONNX model (ran in C++)</li>
						<li>130 MB of storage in total</li>
					</ul>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Now a couple of words on the implementation. Tensorflow, A6000, 50 hours. Since the training is offline, those information are not critical. The most important thing is the embedded models which are ONNX models converted from Tensorflow models, allowing us to run it using C++ libraries on the smartphone. The inference takes 200 ms in total and it requires only 130MB of storage.
					</aside>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Performance: improvement with ensemble learning</h1>
					<hr />
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/ensembling_regression.png" style="width: 75%;"/>
					</div>
					<table class="centred_table fragment fade-down" data-fragment-index="1" style="margin-top: 3%;">
						<thead>
							<tr>
								<th></th>
								<th><b>CNN (mixed)</b></th>
								<th><b>CNN (real)</b></th>
								<th><b>$k$-NN</b></th>
								<th><b>AFA</b></th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>$M$</td>
								<td>0.54</td>
								<td>0.56</td>
								<td>0.57</td>
								<td><b>0.49</b></td>
							</tr>
							<tr>
								<td>$J_0$</td>
								<td>0.28</td>
								<td>0.28</td>
								<td>0.33</td>
								<td><b>0.26</b></td>
							</tr>
							<tr>
								<td>$J_{45}$</td>
								<td>0.21</td>
								<td>0.25</td>
								<td>0.24</td>
								<td><b>0.19</b></td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title fragment fade-down" data-fragment-index="1"><b>Table</b>: Error standard deviation comparison between weak learners and the ensemble learning method</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						We also ran some additional experiments to apply the ensemble learning we previously presented. By ensembling a CNN trained with real data only, a CNN trained with a mix of both real and synthetic data, and a k-NN relying only on synthetic data, ‚û°Ô∏è we can actually improve prediction performance by some margin. Let us precise that the k-NN was a method developed by Essilor.
					</aside>
				</section>
				<section data-transition="fade-in slide-out" class="section_slide">
					<h1>Part II - Deep learning for photorefraction</h1>
					<hr />
					<div style="width: 100%;">
						<div style="float:left; width: 50%;">
							<ol>
								<span class="semi-fade">
									<li>
										Introduction
										<ol>
											<li>Context</li>
											<li>Objectives</li>
											<li>Eye refractive error</li>
										</ol>
									</li>
									<li>
										Proposed pipeline
										<ol>
											<li>Photorefraction principle</li>
											<li>Essilor capture device</li>
											<li>Datasets, CNN architecture & training</li>
											<li>End-to-end pipeline</li>
										</ol>
									</li>
									<li>
										Implementation & performance
										<ol>
											<li>Regression models benchmark</li>
											<li>Real versus synthetic data</li>
											<li>Performance comparison with other device</li>
											<li>Implementation performance</li>
											<li>Ensembling regression models</li>
										</ol>
									</li>
								</span>
								<li>
									Robustness analysis
									<ol>
										<li>Numerical evaluation of robustness</li>
										<li>Improving robustness with a denoiser</li>
										<li>Controlled Lipschitz constant training</li>
									</ol>
								</li>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						As mentioned earlier, since this is a medical device, we need to ensure its reliability. 
					</aside>
				</section>
				<section class="section_slide">
					<h1>Robustness: introduction</h1>
					<hr />
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/pipeline_archi_2.png" style="width: 80%;"/>
					</div>
					<table class="notations_table">
						<tr>
							<td></td>
							<td>$f_{\theta_j}$:</td>
							<td>trained neural network model</td>
						</tr>
						<tr>
							<td></td>
							<td>$x$:</td>
							<td>input tensor of 12 images</td>
						</tr>
					</table>
					<div style="margin-top: 2%;">How to measure the model sensitivity given a perturbation input $b$ ?</div>
					
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						In this section, we will focus on the robustness of our neural network. We will denote them f theta j where j stands for either M J0 or J45. We also denote x the input tensor of 12 images
					</aside>
				</section>
				<section class="section_slide">
					<h1>Robustness: numerical evaluation</h1>
					<hr />
					<div>
						Assuming a small enough input error, and differentiable $f_{\theta_j}$ at $x$,<br />
						<div class="theorem">
							<div class="theorem_title">First-order approximation</div>
							\[
								f_{\theta_j}(x+b) \simeq f_{\theta_j}(x) + \big({\color{teal}\nabla  f_{\theta_j}(x)}\big)^\top b
							\]
						</div>
						Assuming zero-mean Gaussian noise $b$ with covariance matrix $\Lambda_b$, <b>approximation of the output error</b> $e= f_{\theta_j}(x+b)-f_{\theta_j}(x)$, by a zero-mean Gaussian with variance:
						<div class="theorem">
							<div class="theorem_title">Variance of the output error approximation</div>
							\[
							\sigma_e^2(x) = \big({\color{teal}\nabla f_{\theta_j}(x)}\big)^\top \Lambda_b {\color{teal}\nabla f_{\theta_j}(x)}
							\]
						</div>
						<table class="notations_table" style="margin-top: 4%;">
							<tr>
								<td>where</td>
								<td>${\color{teal}\nabla f_{\theta_j}(x)}$:</td>
								<td>network gradient at $x$, easily computed by backpropagation</td>
							</tr>
							<tr>
								<td></td>
								<td>$\Lambda_b$:</td>
								<td>noise covariance, e.g. diagonal matrix with entries $\sigma_b^2$ in the pupil area, and zero elsewhere</td>
							</tr>
						</table>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						We assume that this first order approximation holds for a small enough input error. If the input error is gaussian with covariance Lambda b , we can approximate the output error with a gaussian with the following variance. It can easily be computed with backpropagation 
					</aside>
				</section>
				<section class="section_slide">
					<h1>Robustness: weighted local Lipschitz constant</h1>
					<hr />
					<div class="theorem">
						<div class="theorem_title">Weighted local Lispchitz constant at point $x$ <span id="ref">[Combettes, 2020]</span></div>
						\[
							\sigma_e(x)/\sigma_b
						\]
					</div>
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/lipschitz/cnn_norm1_mask.png" style="width: 80%;"/>
						<div><b>Figure</b>: Histograms of weighted local Lipschitz constants computed with $L_2$ norm on the CNN</div>
					</div>
					<div style="margin-top: 3%;">Reasonably low values of the local weighted Lipschitz constant are a strong indicator of the <b>robustness of the network</b> <span id="ref">[Gupta, 2022]</span>.</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						It allows us to compute the weighted local Lipschitz constant sigma_e of (x) over sigma b for each sample. Below, we see the histograms of those weighted local Lipschitz constants. Values are reasonably low, which indicate the robustness of our networks.
					</aside>
				</section>
				<section class="section_slide">
					<h1>Robustness: improving robustness with a denoiser</h1>
					<hr />
					The application of a denoiser (pretrained DRUnet <span id="ref">[Zhang, 2017]</span>) at test time could help with the robustness <span id="ref">[Aghdam, 2017]</span>.
					<div class="centred_img" style="margin-top: 2%;">
						<img src="res/res_deep_photorefraction/lipschitz/denoised.png" style="width: 80%;"/>
						<div><b>Figure</b>: Example of a denoised pupil image</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						A first method to improve the robustness of our model that we explored is the application of a denoiser at test time. As you can see in this example image, the denoiser will smooth the image and thus remove some possible input perturbations. 
					</aside>
				</section>
				<section class="section_slide">
					<h1>Robustness: improving robustness with a denoiser</h1>
					<hr />
					<table class="centred_table">
						<tr>
							<td></td>
							<td colspan="2">$M$</td>
							<td colspan="2">$J_0$</td>
							<td colspan="2">$J_{45}$</td>
						</tr>
						<tbody>
							<tr style="border-bottom: 1px solid #254456;">
								<td>Lipschitz constant</td>
								<td>Original</td>
								<td>Denoised</td>
								<td>Original</td>
								<td>Denoised</td>
								<td>Original</td>
								<td>Denoised</td>
							</tr>
							<tr>
								<td>count</td>
								<td>906</td>
								<td>906</td>
								<td>906</td>
								<td>906</td>
								<td>906</td>
								<td>906</td>
							</tr>
							<tr>
								<td>mean</td>
								<td>0.61</td>
								<td><b>0.54</b></td>
								<td>1.47</td>
								<td><b>1.28</b></td>
								<td>2.04</td>
								<td><b>1.46</b></td>
							</tr>
							<tr>
								<td>std</td>
								<td><b>0.36</b></td>
								<td>0.41</td>
								<td><b>1.07</b></td>
								<td>1.13</td>
								<td>1.66</td>
								<td><b>1.10</b></td>
							</tr>
							<tr>
								<td>min</td>
								<td>0.05</td>
								<td><b>0.03</b></td>
								<td>0.08</td>
								<td>0.08</td>
								<td>0.05</td>
								<td><b>0.04</b></td>
							</tr>
							<tr>
								<td>25%</td>
								<td>0.34</td>
								<td><b>0.26</b></td>
								<td>0.70</td>
								<td><b>0.57</b></td>
								<td>0.89</td>
								<td><b>0.72</b></td>
							</tr>
							<tr>
								<td>median</td>
								<td>0.53</td>
								<td><b>0.42</b></td>
								<td>1.21</td>
								<td><b>0.97</b></td>
								<td>1.56</td>
								<td><b>1.12</b></td>
							</tr>
							<tr>
								<td>75%</td>
								<td>0.79</td>
								<td><b>0.69</b></td>
								<td>1.95</td>
								<td><b>1.61</b></td>
								<td>2.63</td>
								<td><b>1.87</b></td>
							</tr>
							<tr>
								<td>max</td>
								<td><b>2.45</b></td>
								<td>3.22</td>
								<td><b>7.98</b></td>
								<td>9.11</td>
								<td>11.34</td>
								<td><b>6.81</b></td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title"><b>Table</b>: Weighted local Lipschitz constants comparison between original and denoised images ($L_2$ norm)</div>
					
					<table class="centred_table fragment fade-up" data-fragment-index="1" style="margin-top: 5%;">
						<tr>
							<td></td>
							<td colspan="2">$M$</td>
							<td colspan="2">$J_0$</td>
							<td colspan="2">$J_{45}$</td>
						</tr>
						<tbody>
							<tr style="border-bottom: 1px solid #254456;">
								<td>Error</td>
								<td>Raw</td>
								<td>Denoised</td>
								<td>Raw</td>
								<td>Denoised</td>
								<td>Raw</td>
								<td>Denoised</td>
							</tr>
							<tr>
								<td>mean</td>
								<td><b>-0.04</b></td>
								<td>0.29</td>
								<td><b>-0.02</b></td>
								<td>0.03</td>
								<td><b>0.01</b></td>
								<td>0.05</td>
							</tr>
							<tr>
								<td>std</td>
								<td><b>0.56</b></td>
								<td>0.75</td>
								<td><b>0.27</b></td>
								<td>0.33</td>
								<td><b>0.22</b></td>
								<td>0.29</td>
							</tr>
							<tr>
								<td>min</td>
								<td>-2.17</td>
								<td><b>-1.91</b></td>
								<td><b>-0.89</b></td>
								<td>-1.31</td>
								<td><b>-0.81</b></td>
								<td>-0.94</td>
							</tr>
							<tr>
								<td>max</td>
								<td><b>2.50</b></td>
								<td>2.92</td>
								<td><b>0.98</b></td>
								<td>1.34</td>
								<td><b>1.18</b></td>
								<td>1.69</td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title fragment fade-up" data-fragment-index="1"><b>Table</b>: Performance comparison between original and denoised images</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						As shows this table, a substantial improvement in the values of the weighted local Lipschitz constants is visible: in average, for M it goes from 0.61 to 0.54, from 1.47 to 1.28 for J0 and from 2.04 to 1.46 for J 45.<br /><br />

						However, the drawback is of course that, by smoothing images to remove input perturbations, some useful information can also be removed in the process.<br /><br />
						‚û°Ô∏è This is observable in this second table where we see that a slight decrease in prediction performance occurs when applying such denoiser, with the biggest increase being for M which goes from 0.56 in standard deviation to 0.75. 
					</aside>
				</section>
				<section class="section_slide">
					<h1>Robustness: controlled Lipschitz constant training</h1>
					<hr />
					Using the deel-lip library <span id="ref">[Serrurier, 2020]</span>,
					<div class="centred_img" style="margin-top: 2%;">
						<img src="res/res_deep_photorefraction/lipschitz/perf.png" style="width:95%" />
						<div><b>Figure</b>: Error standard deviation of a model trained with deel-lip in function of $K$</div>
					</div>
					<div style="margin-top: 5%;">
						The lower the Lipschitz constant, the greater the constraints. <br />
						‚û°Ô∏è balance robustness and performance
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						We present now a second method that we explored that should give us a finer control over the Lipschitz constant of the neural network. However, this method requires to retrain all models from scratch using a specific library called deel lip. This library allows us to set beforehand the targeted Lipschitz constant and will then constraint the model during training always to satisfy this requirement.<br /><br />
						
						the lower the Lipschitz constant, the greater the constraints and hence the lower the performance: this graph illustrates it as we can see the error standard deviation which increases as we decrease the Lipschitz constant. This method allows us to better balance robustness and pure prediction performance. 
					</aside>
				</section>
				<section class="section_slide">
					<h1>Robustness: controlled Lipschitz constant training</h1>
					<hr />
					<div style="margin-top: 2%;">Using a CNN trained with a Lipschitz constant below 4 (4-LCNN),</div>
					<table class="centred_table" style="margin-top: 10%;">
						<tr>
							<th></th>
							<th colspan="2">$M$</th>
							<th colspan="2">$J_0$</th>
							<th colspan="2">$J_{45}$</th>
						</tr>
						<tbody>
							<tr style="border-bottom: 1px solid #254456;">
								<td></td>
								<td>CNN</td>
								<td>4-LCNN</td>
								<td>CNN</td>
								<td>4-LCNN</td>
								<td>CNN</td>
								<td>4-LCNN</td>
							</tr>
							<tr>
								<td>mean</td>
								<td>-0.04</td>
								<td>0.06</td>
								<td>-0.02</td>
								<td>0.05</td>
								<td>0.01</td>
								<td>0.01</td>
							</tr>
							<tr>
								<td>std</td>
								<td>0.56</td>
								<td>0.70</td>
								<td>0.27</td>
								<td>0.37</td>
								<td>0.22</td>
								<td>0.24</td>
							</tr>
							<tr>
								<td>min</td>
								<td>-2.17</td>
								<td>-2.63</td>
								<td>-0.89</td>
								<td>-1.13</td>
								<td>-0.81</td>
								<td>-0.83</td>
							</tr>
							<tr>
								<td>max</td>
								<td>2.50</td>
								<td>4.23</td>
								<td>0.98</td>
								<td>1.36</td>
								<td>1.18</td>
								<td>0.97</td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title"><b>Table</b>: Performance comparison between the 4-LCNN and the standard CNN, on the dataset of real images</div>		
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						We chose a CNN constrained to have a Lipschitz constant below 4 as it is the lowest value for which the models fit the performance requirements. 
					</aside>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Robustness: controlled Lipschitz constant training</h1>
					<hr />
					<div class="centred_img" style="margin-top: 1%;">
						<img src="res/res_deep_photorefraction/lipschitz/cnn_norm1_mask.png" style="width:70%" />
						<div><b>Figure</b>: Histograms of weighted local Lipschitz constants computed with $L_2$ norm on the standard CNN</div>
					</div>
					<div class="centred_img" style="margin-top: 1%;">
						<img src="res/res_deep_photorefraction/lipschitz/deelcnn_norm1_mask.png" style="width:70%" />
						<div><b>Figure</b>: Histograms of weighted local Lipschitz constants computed with $L_2$ norm on the 4-LCNN</div>
					</div>	
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						These histograms show that the weighted local Lipschitz constants have much lower value for the CNN with a Lipschitz constant below 4 than the standard CNN (please be mindful of the different scale on the x-axis).
					</aside>
				</section>
				<section data-transition="fade" class="section_slide">
					<h1>Part II - Conclusion</h1>
					<hr />
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/pipeline_archi.png" style="width: 80%; margin-top: 1%;"/>
					</div>
					<table class="fragment fade-in" data-fragment-index="0" style="margin-top: 1%;">
						<tr>
							<td>Design a screening tool</td>
							<td></td>
							<td></td>
						</tr>
						<tr>
							<td><li>accessible</li></td>
							<td class="fragment fade-right" data-fragment-index="1">‚û°Ô∏è</td>
							<td class="fragment fade-right" data-fragment-index="1">smartphone application</td>
						</tr>
						<tr>
							<td><li>affordable</li></td>
							<td class="fragment fade-right" data-fragment-index="1">‚û°Ô∏è</td>
							<td class="fragment fade-right" data-fragment-index="1">low-priced device</td>
						</tr>
						<tr>
							<td><li>with competitive performance</li></td>
							<td class="fragment fade-right" data-fragment-index="1">‚û°Ô∏è</td>
							<td class="fragment fade-right" data-fragment-index="1">outperforms competition</td>
						</tr>
						<tr>
							<td><li>reliable</li></td>
							<td class="fragment fade-right" data-fragment-index="1">‚û°Ô∏è</td>
							<td class="fragment fade-right" data-fragment-index="1">controllable Lipschitz values</td>
						</tr>
					</table>
					<div class="fragment fade-in" data-fragment-index="2" style="margin-top: 2.5%; border-left: 5px solid #254456; padding-left: 1em;">
						M. Vu, √â. Chouzenoux, J.-C. Pesquet, S. Boutinon, M. Peloux, P. Pinault, Deep Learning Method For Accessible Eccentric Photorefraction, <em>Proceedings of the IEEE International Symposium on Biomedical Imaging (ISBI)</em>, Athens, Greece, 27-30 May 2024, pp. 1-5.
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Let's now conclude this second part of the presentation. We proposed a novel photorefraction pipeline based on deep learning to perform the regression of parameters M J0 and J45 given an input of 12 images captured by the device designed by Essilor.<br /><br />
						‚û°Ô∏è At the beginning of the presentation, we listed some requirements for our system. ‚û°Ô∏è We needed it to be accessible, we showed that it is thanks to a smartphone application, it is also more affordable than the direct competition and all of that without sacrificing its screening performance<br /><br />
						and finally, we measure its reliability using Lipschitz constants as well as methods to have control over it.
					</aside>
				</section>
				<section data-transition="fade" class="section_slide">
					<h1>Part I - Ensemble learning</h1>
					<div class="header_hr" data-id="header_hr">
						<hr />
					</div>
					<div style="width: 100%;">
						<div style="float:left; width: 50%;" data-id="left_col">
							<h1 class="fragment fade-down" style="font-size: 1.6em;">Conclusion</h1>
							<ul>
								<li class="fragment fade-down">General framework for describing averaging rules in ensemble learning
									<ul>
										<li>Focus on output fusion</li>
									</ul>
								</li>
								<li style="padding-top: 2em;" class="fragment fade-down">Optimal ensembling through training of an interpretable neural network architecture
									<ul>
										<li>$f$-average: neural network modelling an average</li>
										<li>Aggregating multiple $f$-averages</li>
										<li>Stacking multiple aggregated $f$-averages</li>
									</ul>
								</li>
								<li style="padding-top: 2em;" class="fragment fade-down">Application to few-shot class incremental learning
									<ul>
										<li>Best performance among ensemble learning methods</li>
										<li>Comparable performance with specialised state-of-the-art methods</li>
									</ul>
								</li>
							</ul>
						</div>
						<div style=" float:right; width: 50%;">
							<h1 class="fragment fade-down" style="font-size: 1.6em;">Perspectives</h1>
							<ul>
								<li class="fragment fade-down">$f$-average: scenario illustrating its interpretable weights</li>
								<li style="padding-top: 2em;" class="fragment fade-down">FSCIL: open-set recognition methods as weak classifiers</li>
							</ul>
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						<div>
							I will now conclude the whole presentation.
							
							In the first part on ensemble learning, we presented a method inspired by averaging rules to perform the output fusion step of an ensembling method. We proposed an architecture which can model and aggregate multiple types of averages by choosing adequates activation functions and by imposing some constraints on weight matrices. Finally, we used the few shot class incremental learning setting as it is a problem well suited to illustrate the output fusion method that we propose. In terms of performance, we show that our method outperforms standard ensembling methods such as averages and stacked models partly thanks to an increased training stability and we also show that altough not the best, our method performance are comparable with state of the art methods designed specifically for the problem of few shot class incremental learning.<br /><br />

							We also identified some work perspectives that could be carried out in order to either improve or deepen our work. First, identifying a use case or a scenario to showcase the interpretability properties of the f-average model. This would be useful for example in a medical context where a deep learning based tool would help a medical expert to identify the correct diagnosis.<br /><br />

							For the few shot class incremental learning problem, we could improve weak learners by using open-set recognition models to replace the more simple nearest neighbor classifier. This would provide a finer inlierness prediction for the padding values.<br /><br />
						</div>
					</aside>
				</section>
				<section data-transition="fade" class="section_slide">
					<h1>Part II - Deep learning for photorefraction</h1>
					<div class="header_hr" data-id="header_hr">
						<hr />
					</div>
					<div style="width: 100%;">
						<div style="float:left; width: 50%;" data-id="left_col">
							<h2 class="fragment fade-down">Conclusion</h2>
							<ul>
								<li class="fragment fade-down">
									A novel pipeline for photorefraction based on deep learning
									<ul>
										<li>A capture device</li>
										<li>Regression through CNN models</li>
										<li>Smartphone control </li>
									</ul>
								</li>
								<li class="fragment fade-down" style="padding-top: 2em;">
									Performance validation
									<ul>
										<li>Tested on real and synthetic data</li>
										<li>Outperforms competing device</li>
										<li>Performance improvement with ensembling</li>
									</ul>
								</li>
								<li class="fragment fade-down" style="padding-top: 2em;">
									Robustness analysis
									<ul>
										<li>Robustness evaluation using Lipschitz constant</li>
										<li>Methods to control & improve robustness</li>
									</ul>
								</li>
							</ul>
						</div>
						<div style=" float:right; width: 50%;">
							<h2 class="fragment fade-down">Perspectives</h2>
							<ul>
								<li class="fragment fade-down">Generative model for image to image translation: transform synthetic into real images</li>
								<li class="fragment fade-down" style="padding-top: 2em;">Deep learning-based out-of-range detection tool & invalid capture detection</li>
								<li class="fragment fade-down" style="padding-top: 2em;">New acquisition campaign & industrial deployment</li>
							</ul>
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						In the second part, as we just saw, we proposed a novel photorefraction pipeline based on deep learning to perform the regression of parameters M J0 and J45 given an input of 12 images captured by the device designed by Essilor. The whole system is controlled by a smartphone. We validate the performance on real as well as synthetic data and showed that the system outperforms a competing device. We also showed that a performance boost could be achieved using ensemble learning. Finally, since this is a medical diagnosis device, we focus on an important aspect which is its robustness. We evaluated it using the Lipschitz constant and also proposed methods to improve the robustness.<br /><br />

						For our photorefraction work, we could further explore a work that was initiated with an intern who worked with us on this project: the use of generative models for image to image translation that would transform synthetic images into real or at least more realistic images.<br /><br />

						Around the core pipeline that we presented, the are other opportunities to use deep learning in order to improve the overall screening tool: for example, including a Deep learning-based out-of-range detection tool & invalid capture detection could benefit the end user.<br /><br />

						Finally, this project will still be developing, with a new acquisition campaign that will add data (which is as you know essential) to the training sets and with a future industrial deployment in perspective.
					</aside>
				</section>
				<section data-transition="fade" class="section_slide">
					<h1>References</h1>
					<div class="header_hr" data-id="header_hr">
						<hr />
					</div>
					<table id="ref_table">
						<tr>
							<td>[Galton, 1907]</td>
							<td>Galton, F. (1907). Vox populi (the wisdom of crowds). <em>Nature</em>, 75(7):450‚Äì451.</td>
						</tr>
						<tr>
							<td>[Kolmogorov, 1930]</td>
							<td>Kolmogorov, A. (1930). <em>Mathematics and Mechanics</em>. Kluwer.</td>
						</tr>
						<tr>
							<td>[Wang, 2019]</td>
							<td>Wang, Y., Chao, W., Weinberger, K. Q., and van der Maaten, L. (2019). Simpleshot: Revisiting nearest-neighbor classification for few-shot learning. DOI: 10.48550/arxiv.1911.04623.</td>
						</tr>
						<tr>
							<td>[Castro, 2018]</td>
							<td>Castro, F. M., Mar√≠n-Jim√©nez, M. J., Guil, N., Schmid, C., and Alahari, K. (8th September - 14th September 2018). End-to-end incremental learning. In <em>Proceedings of the European Conference on Computer Vision (ECCV)</em>, pages 233‚Äì248, Munich, Germany. DOI: 10.1007/978-3-030-01258-8_15.</td>
						</tr>
						<tr>
							<td>[Hou, 2019]</td>
							<td>Hou, S., Pan, X., Loy, C. C., Wang, Z., and Lin, D. (16th June - 20th June 2019). Learning a unified classifier incrementally via rebalancing. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 831‚Äì839, Long Beach, California. DOI: 10.1109/cvpr.2019.00092.</td>
						</tr>
						<tr>
							<td>[Zhu, 2021]</td>
							<td>Zhu, K., Cao, Y., Zhai, W., Cheng, J., and Zha, Z.-J. (19th June - 25th June 2021). Self-promoted prototype refinement for few-shot class-incremental learning. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 6801‚Äì6810, (online). DOI: 10.1109/cvpr46437.2021.00673.</td>
						</tr>
						<tr>
							<td>[Zhang, 2021]</td>
							<td>Zhang, C., Song, N., Lin, G., Zheng, Y., Pan, P., and Xu, Y. (19th June - 25th June 2021). Few-shot incremental learning with continually evolved classifiers. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 12455‚Äì12464, (online). DOI: 10.1109/cvpr46437.2021.01227.</td>
						</tr>
						<tr>
							<td>[Zhou, 2022]</td>
							<td>Zhou, D.-W., Wang, F.-Y., Ye, H.-J., Ma, L., Pu, S., and Zhan, D.-C. (19th June - 24th June 2022). Forward compatible few-shot class-incremental learning. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 9046‚Äì9056, New Orleans, Louisiana. DOI: 10.1109/cvpr52688.2022.00884.</td>
						</tr>
						<tr>
							<td>[Holden, 2016]</td>
							<td>Holden, B. A., Fricke, T. R., Wilson, D. A., Jong, M., Naidoo, K. S., Sankaridurg, P., Wong, T. Y., Naduvilath, T. J., and Resnikoff, S. (2016). Global prevalence of myopia and high myopia and temporal trends from 2000 through 2050. <em>Ophthalmology</em>, 123(5):1036‚Äì1042, DOI: 10.1016/j.ophtha.2016.01.006.</td>
						</tr>
						<tr>
							<td>[Gekeler, 1997]</td>
							<td>Gekeler, F., Schaeffel, F., Howland, H. C., and Wattam-Bell, J. (1997). Measurement of astigmatism by automated infrared photoretinoscopy. <em>Optometry and Vision Science</em>, 74(7):472‚Äì482, DOI: 10.1097/00006324-199707000-00013.</td>
						</tr>
						<tr>
							<td>[Martin, 2020]</td>
							<td>Martin, S. J., Htoo, H. E., Hser, N., and Arnold, R. W. (2020). Performance of two photoscreeners enhanced by protective cases. <em>Clinical Ophthalmology</em>, page 1427‚Äì1435.</td>
						</tr>
						<tr>
							<td>[Kurent, 2022]</td>
							<td>Kurent, A. (2022). Comparison of the cycloplegic refractive measurements with handheld, table-mounted refractometers and retinoscopy in children. <em>Ophthalmology Journal</em>, 7:200‚Äì207, DOI: 10.5603/oj.2022.0028.</td>
						</tr>
						<tr>
							<td>[Combettes, 2020]</td>
							<td>Combettes, P. L. and Pesquet, J.-C. (2020). Lipschitz certificates for neural network structures driven by averaged activation operators. <em>SIAM Journal on Mathematics of Data Science</em>, 2:529‚Äì557.</td>
						</tr>
						<tr>
							<td>[Gupta, 2022]</td>
							<td>Gupta, K., Kaakai, F., Pesquet-Popescu, B., Pesquet, J.-C., and Malliaros, F. D. (2022). Multivariate Lipschitz analysis of the stability of neural networks. <em>Frontiers in Signal Processing</em>, 2, ISSN: 2673-8198, DOI: 10.3389/frsip.2022.794469.</td>
						</tr>
						<tr>
							<td>[Zhang, 2017]</td>
							<td>Zhang, K., Zuo, W., Gu, S., and Zhang, L. (21st July - 26th July 2017). Learning deep cnn denoiser prior for image restoration. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 2808‚Äì2817, Honolulu, Hawaii. DOI: 10.1109/CVPR.2017.300.</td>
						</tr>
						<tr>
							<td>[Aghdam, 2017]</td>
							<td>Aghdam, H. H., Heravi, E. J., and Puig, D. (27th February - 1st March 2017). Increasing the stability of cnns using a denoising layer regularized by local lipschitz constant in roadunderstanding problems. In <em>Proceedings of the International Joint Conference on Computer Vision, Imaging and ComputerGraphics Theory and Applications (VISIGRAPP)</em>, pages 218‚Äì225, Porto, Portugal. INSTICC, SciTePress, ISBN: 978-989-758-226-4, DOI: 10.5220/0006123602180225.</td>
						</tr>
						<tr>
							<td>[Serrurier, 2020]</td>
							<td>Serrurier, M., Mamalet, F., Gonz√°lez-Sanz, A., Boissin, T., Loubes, J.M., and del Barrio, E. (2020). Achieving robustness in classification using optimal transport with hinge regularization. DOI: 10.48550/arxiv.2006.06520.</td>
						</tr>
						
					</table>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
						Here are the references. Thank you for listening and now, if you have any, I will take your questions.
					</aside>
				</section>
				<section data-transition="fade">
					<div class="centred_img" style="margin-top:13%">
						<!-- <img src="res/res_deep_photorefraction/device.png" /> -->
						<div style="margin-top:5%"><h1>Thank you for listening !</h1></div>
					</div>
				</section>
				<section data-transition="fade" class="section_slide">
					<h1>Summary</h1>
					<div class="header_hr" data-id="header_hr">
						<hr />
					</div>
					<div style="width: 100%;">
						<div style="float:left; width: 50%;" data-id="left_col">
							<h2>Part I - Ensemble learning</h2>
							<ol>
								<li>
									<a class="summary_link" href="#/2">Introduction</a>
									<ol>
										<a class="summary_link" href="#/3"><li>Wisdom of the crowd</li></a>
										<a class="summary_link" href="#/4"><li>Ensemble learning</li></a>
										<a class="summary_link" href="#/5"><li>Generalised mean</li></a>
									</ol>
								</li>
								<li>
									<a class="summary_link" href="#/6">Proposed model</a>
									<ol>
										<a class="summary_link" href="#/7"><li>$f$-average</li></a>
										<a class="summary_link" href="#/9"><li>Aggregated $f$-averages</li></a>
										<a class="summary_link" href="#/10"><li>Stacked aggregated $f$-averages</li></a>
										<a class="summary_link" href="#/11"><li>Application example</li></a>
									</ol>
								</li>
								<li>
									<a class="summary_link" href="#/12">Application to Few-Shot Class Incremental Learning</a>
									<ol>
										<a class="summary_link" href="#/14"><li>Problem setting</li></a>
										<a class="summary_link" href="#/15"><li>Proposed approach</li></a>
										<a class="summary_link" href="#/19"><li>Comparative results</li></a>
									</ol>
								</li>
							</ol>
						</div>
						<div style=" float:right; width: 50%;">
							<h2>Part II - Deep learning for photorefraction</h2>
							<ol>
								<li>
									<a class="summary_link" href="#/21">Introduction</a>
									<ol>
										<a class="summary_link" href="#/23/0/1"><li>Context</li></a>
										<a class="summary_link" href="#/25"><li>Objectives</li></a>
										<a class="summary_link" href="#/26"><li>Eye refractive error</li></a>
									</ol>
								</li>
								<li>
									<a class="summary_link" href="#/27">Proposed pipeline</a>
									<ol>
										<a class="summary_link" href="#/28"><li>Photorefraction principle</li></a>
										<a class="summary_link" href="#/30"><li>Essilor capture device</li></a>
										<a class="summary_link" href="#/31"><li>Datasets, CNN architecture & training</li></a>
									</ol>
								</li>
								<li>
									<a class="summary_link" href="#/35">Implementation & performance</a>
									<ol>
										<a class="summary_link" href="#/36"><li>Regression models benchmark</li></a>
										<a class="summary_link" href="#/37"><li>Real versus synthetic data</li></a>
										<a class="summary_link" href="#/39"><li>Performance comparison with other device</li></a>
										<a class="summary_link" href="#/40"><li>Implementation performance</li></a>
										<a class="summary_link" href="#/41/0/0"><li>Ensembling regression models</li></a>
									</ol>
								</li>
								<li>
									<a class="summary_link" href="#/42">Robustness analysis</a>
									<ol>
										<a class="summary_link" href="#/44"><li>Numerical evaluation of robustness</li></a>
										<a class="summary_link" href="#/46"><li>Improve robustness with a denoiser</li></a>
										<a class="summary_link" href="#/48"><li>Controlled Lipschitz constant training</li></a>
									</ol>
								</li>
							</ol>
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
					<aside class="notes">
					</aside>
				</section>
			</div>
		</div>
		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			Reveal.initialize({
				width: '90%',
				height: '90%',
				// width: '100%',
				// height: '100%',
				controls: false,
				hash: true,
				center: false,
				mouseWheel: true,
				slideNumber: 'true',
				plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX],
				pdfSeparateFragments: false,
			});
		</script>
	</body>
</html>
