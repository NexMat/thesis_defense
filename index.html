<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
		<title>Novel deep learning approaches for image analysis</title>
		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/custom.css">
		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-transition="fade-out" class="center" style="color: #254456;">
					<div style="width: 100%;">
						<div style="float:left; width: 35%; margin-top: 10%;">
							<img src="res/za/logo_opis.png" alt="Logos OPIS">
						</div>
						<div style="float:right; width: 64%;">
							<h1>Novel Deep Learning Approaches For Image Analysis</h1>
							<h2>Mathieu Vu</h2>
							<em>Ph.D. defense - 28th November 2024</em>
							<table class="left_aligned" style="margin-top:5%;">
								<tr>
									<td><em>Supervisors:</em></td>
								</tr>
								<tr>
									<td><b>Émilie Chouzenoux</b></td>
									<td>Research Director (Inria, France)</td>
								</tr>
								<tr>
									<td><b>Philippe Pinault</b></td>
									<td>R&D Engineer (EssilorLuxottica, France)</td>
								</tr>
								<tr>
									<td style="opacity: 0%;">-</td>
								</tr>
								<tr>
									<td><em>Jury members:</em></td>
								</tr>
								<tr>
									<td><b>Amel Benazza</b></td>
									<td>Professor (SUP’COM Tunis, Tunisia)</td>
								</tr>
								<tr>
									<td><b>Adrian Basarab</b></td>
									<td>Professor (Université de Lyon, France)</td>
								</tr>
								<tr>
									<td><b>Benjamin Guedj</b></td>
									<td>Research Director (Inria, UCL, United Kingdom)</td>
								</tr>
								<tr>
									<td><b>Céline Hudelot</b></td>
									<td>Professor (CentraleSupélec, France)</td>
								</tr>
								<tr>
									<td><b>Ismail Ben Ayed</b></td>
									<td>Professor (École de Technologie Supérieure, Canada)</td>
								</tr>
							</table>
						</div>
					</div>
				</section>
				<section data-auto-animate data-transition="fade-in none-out" class="section_slide">
					<h1>Summary</h1>
					<div class="header_hr" data-id="header_hr">
						<hr />
					</div>
					<div style="width: 100%;">
						<div style="float:left; width: 50%;" data-id="left_col">
							<h1 class="fragment fade-down" data-fragment-index="1" style="font-size: 1.6em;">Part I - Ensemble learning</h1>
							<ol>
								<li class="fragment fade-down" data-fragment-index="2">
									Introduction
									<ol>
										<li>Wisdom of the crowd</li>
										<li>Ensemble learning</li>
										<li>Generalised mean</li>
									</ol>
								</li>
								<li class="fragment fade-down" data-fragment-index="3">
									Proposed model
									<ol>
										<li>$f$-average</li>
										<li>Aggregated $f$-averages</li>
										<li>Stacked aggregated $f$-averages</li>
									</ol>
								</li>
								<li class="fragment fade-down" data-fragment-index="4">
									Application to Few-Shot Class Incremental Learning
									<ol>
										<li>Problem setting</li>
										<li>Proposed approach</li>
										<li>Comparative results</li>
									</ol>
								</li>
							</ol>
						</div>
						<div style=" float:right; width: 50%;">
							<h2 class="fragment fade-down" data-fragment-index="5">Part II - Deep learning for photorefraction</h2>
							<ol>
								<li class="fragment fade-down" data-fragment-index="6">
									Introduction
									<ol>
										<li>Context</li>
										<li>Objectives</li>
										<li>Eye refractive error</li>
									</ol>
								</li>
								<li class="fragment fade-down" data-fragment-index="7">
									Proposed pipeline
									<ol>
										<li>Photorefraction principle</li>
										<li>Essilor capture device</li>
										<li>Objectives</li>
										<li>Datasets, CNN architecture & training</li>
									</ol>
								</li>
								<li class="fragment fade-down" data-fragment-index="8">
									Implementation & performance
									<ol>
										<li>Regression models benchmark</li>
										<li>Real versus synthetic data</li>
										<li>Performance comparison with other device</li>
										<li>Implementation performance</li>
										<li>Ensembling regression models</li>
									</ol>
								</li>
								<li class="fragment fade-down" data-fragment-index="9">
									Robustness analysis
									<ol>
										<li>Introduction on Lipschitz constant</li>
										<li>Improve robustness with a denoiser</li>
										<li>Controlled Lipschitz constant training</li>
									</ol>
								</li>
							</ol>
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-auto-animate data-transition="none-in slide-out" data-auto-animate-unmatched="false" class="section_slide">
					<h1>Part I - Ensemble learning</h1>
					<div class="header_hr" data-id="header_hr">
						<hr />
					</div>
					<div style="width: 100%;">
						<div style="float:left; width: 50%;" data-id="left_col">
							<h2 style="opacity: 0%;">.</h2>
							<ol>
								<li>
									Introduction
									<ol>
										<li>Wisdom of the crowd</li>
										<li>Ensemble learning</li>
										<li>Generalised mean</li>
									</ol>
								</li>
								<span class="semi-fade">
									<li>
										Proposed model
										<ol>
											<li>$f$-average</li>
											<li>Aggregated $f$-averages</li>
											<li>Stacked aggregated $f$-averages</li>
										</ol>
									</li>
									<li>
										Application to Few-Shot Class Incremental Learning
										<ol>
											<li>Motivation & problem setting</li>
											<li>Proposed approach</li>
											<li>Comparative results</li>
										</ol>
									</li>
								</span>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr class="footer_hr" data-id="footer_hr" />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Introduction: wisdom of the crowd</h1>
					<hr />
					<div class="centred_img">
						<img src="res/hoxton_market_shoreditch_1910.jpg" style="width:45%" />
						<div>British market, 1910</div>
					</div>
					<div style="margin-top: 1%;">
						Francis Galton's study of a weight-guessing contest [1]: <br />
						<ul>
							<li>800 participants</li>
							<li>guesses mean: 1,208 pounds</li>
							<li>ox weight: 1,197 pounds</li>
						</ul>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Introduction: ensemble learning</h1>
					<hr />
					<div class="centred_img">
						<img src="res/ensemble_learning.jpg" style="width:55%" />
						<div>where $(\forall k \in \{1,\ldots, K\}), x_k \in \mathbb{R}^N$ and $\widetilde{x} \in
							\mathbb{R}^N$
						</div>
					</div>
					<div style="margin-top: 7%;">
						<b>Goal 🎯</b>: leverage multiple models to enhance performance on a specific task
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Introduction: generalised average</h1>
					<hr />
					<div>
						Consider $K$ estimates $(x_k)_{1 \le k \le K}$ and let $\widetilde{x}$ in $[0, +\infty[$ be the predicted output after ensembling. <br />
					</div>
					<div style="margin-top: 1%;">
						<b>Standard average rules:</b> <br />
						<table style="margin: 0;" class="table_means">
							<tr>
								<td>Arithmetic:</td>
								<td>$\widetilde{x} = \sum_{k=1}^K \omega_{k}x_{k}$</td>
							</tr>
							<tr>
								<td>Geometric:</td>
								<td>$\widetilde{x} = \prod_{k=1}^K x_{k}^{\omega_{k}}$</td>
							</tr>
							<tr>
								<td>Harmonic:</td>
								<td>$\widetilde{x} = (\sum_{k=1}^K \frac{\omega_{k}}{x_{k}})^{-1}$</td>
							</tr>
							<tr>
								<td>Power-$q$:</td>
								<td>$\widetilde{x} = \left( \sum_{k=1}^K \omega_{k} x_{k}^q \right)^{1/q}$</td>
							</tr>
						</table>
						where $(\omega_k)_{1 \le k \le K}$ are some weights such that $\sum_{k=1}^K \omega_k = 1$.
					</div>
					<div class="theorem">
						<div class="theorem_title">Kolmogorov's generalised average [2]</div>
						$\widetilde{x} = f^{-1}\Big(\sum_{k=1}^K \omega_{k} f(x_{k})\Big)$
					</div>
					with $(\forall k \in \{1,\ldots, K\}), x_k \in [0, +\infty[^N$, $\widetilde{x} \in [0, +\infty[^N$, and
					$f:[0, +\infty[^N \mapsto \mathbb{R}^{N}$ bijective, with inverse $f^{-1}$. <br /><br />
					🤔 How to learn the optimal averaging rule/weights?
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
				</section>
				<section data-transition="fade-in slide-out" class="section_slide">
					<h1>Part I - Ensemble learning</h1>
					<hr />
					<div style="width: 100%;">
						<div style="float:left; width: 50%;">
							<h2 style="opacity: 0;">-</h2>
							<ol>
								<span class="semi-fade">
									<li>
										Introduction
										<ol>
											<li>Wisdom of the crowd</li>
											<li>Ensemble learning</li>
											<li>Generalised mean</li>
										</ol>
									</li>
								</span>
								<li>
									Proposed model
									<ol>
										<li>$f$-average</li>
										<li>Aggregated $f$-averages</li>
										<li>Stacked aggregated $f$-averages</li>
										
									</ol>
								</li>
								<span class="semi-fade">
									<li>
										Application to Few-Shot Class Incremental Learning
										<ol>
											<li>Motivation & problem setting</li>
											<li>Proposed approach</li>
											<li>Comparative results</li>
										</ol>
									</li>
								</span>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Proposed model: $f$-average</h1>
					<hr />
					<div class="theorem">
						<div class="theorem_title">Kolmogorov's generalised average [2]</div>
						$\widetilde{x} = f^{-1}\Big(\sum_{k=1}^K \omega_{k} f(x_{k})\Big)$
					</div>
					<div>
						with $(\forall k \in \{1,\ldots, K\}), x_k \in [0, +\infty[^N$, $\widetilde{x} \in [0, +\infty[^N$, and
						$f:[0, +\infty[^N \mapsto \mathbb{R}^{N}$ bijective, with inverse $f^{-1}$.
					</div>
					<div style="margin-top: 5%;" class="theorem">
						<div class="theorem_title">Further generalisation: $f$-average</div>
						$\widetilde{x} = f^{-1}\Big(W \boldsymbol{f}(\boldsymbol{x})\Big)$
					</div>
					<br />
					$\boldsymbol{x}$: $\in \mathbb{R}^{KN}$, vertical concatenation of the inputs $(x_k)_{1 \le k \le K}$
					<br /><br />
					$\boldsymbol{f}$: function generalising $f$ operating componentwise from $[0, +\infty[^{KN}$ to
					$\mathbb{R}^{KN}$
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Proposed model: NN modelling of an $f$-average</h1>
					<hr />
					<div class="centred_img">
						<img src="res/f-average.png" style="width:55%" />
						<div><b>Figure</b>: Structure of a neural network modelling an $f$-average</div>
					</div>
					<div style="margin-top: 2%;">An optimal ensembling rule based on the average encoded in $(f,f^{-1})$ can
						be obtained through supervised learning of matrix $W$.
					</div>
					<table class="centred_table" style="margin-top: 10%;">
						<thead>
							<tr>
								<th>Mean</th>
								<th>$f(x)$</th>
								<th>$f$ domain</th>
								<th>$f^{-1}(x)$</th>
								<th>$f^{-1}$ domain</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>Arithmetic</td>
								<td>$Id$</td>
								<td>$[0,+\infty[^N$</td>
								<td>$Id$</td>
								<td>$[0,+\infty[^N$</td>
							</tr>
							<tr>
								<td>Geometric</td>
								<td>$\big(\ln(\xi_{n}+\epsilon) \big)_{1\le n \le N}$</td>
								<td>$[0,+\infty[^N$</td>
								<td>$\big(\exp(\xi_{n})-\epsilon\big)_{1\le n\le N}$</td>
								<td>$[\ln(\epsilon),+\infty[^N$</td>
							</tr>
							<tr>
								<td>Harmonic</td>
								<td>$\big(h_{\epsilon}(\xi_{n})\big)_{1\le n \le N}$</td>
								<td>$[0,+\infty[^N$</td>
								<td>$\big(h_{\epsilon}(\xi_{n})\big)_{1\le n \le N}$</td>
								<td>$]-\infty,\epsilon^{-1} - \epsilon]^N$</td>
							</tr>
							<tr>
								<td>Quadratic</td>
								<td>$x^2$</td>
								<td>$[0,+\infty[^N$</td>
								<td>$\sqrt{x}$</td>
								<td>$[0,+\infty[^N$</td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title">
						<b>Table</b>: Examples for $f$ and $f^{-1}$, and definition domains.
						For geometric and harmonic means, classical mean formulas are retrieved when $\epsilon \to 0$.
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Proposed model: aggregated $f$-averages</h1>
					<hr />
					<div class="centred_img">
						<img src="res/aggregated-f-average.png" style="width:30%" />
						<div><b>Figure</b>: Structure of a neural network aggregating $J$ $f$-averages</div>
					</div>
					<table class="notations_table" style="margin-top: 3%;">
						<tr>
							<td>$\forall j \in \{1,\cdots,J\}$,</td>
							<td>$\boldsymbol{x_j}$:</td>
							<td>$\in \mathbb{R}^{KN}$, vertical concatenation of the inputs $(x_k)_{1 \le k \le K}$</td>
						</tr>
						<tr>
							<td></td>
							<td>$\boldsymbol{f_j}$:</td>
							<td>activation function operating componentwise from $[0, +\infty[^{KN}$ to $\mathbb{R}^{KN}$</td>
						</tr>
						<tr>
							<td></td>
							<td>$W_j$:</td>
							<td>$\in [0, + \infty[^{N\times KN}$ such that $W1\kern-0.25em\text{l}_{KN} = 1\kern-0.25em\text{l}_{N}$</td>
						</tr>
						<tr>
							<td></td>
							<td>$f^{-1}_j$:</td>
							<td>inverse activation function operating componentwise from $\mathbb{R}^{N}$ to $[0, +\infty[^{N}$</td>
						</tr>
						<tr>
							<td></td>
							<td>$A$:</td>
							<td>$\in \mathbb{R}^{N \times NJ}$</td>
						</tr>
					</table>
					<div style="margin-top: 2%;">
						➡️ This allows to learn the optimal balance between $J$ different average rules, through the
						learning of $(W_j)_{1\leq j \leq J}$ and $A$.
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Proposed model: stacked aggregated $f$-averages</h1>
					<hr />
					<div class="centred_img">
						<img src="res/stacked-aggregated-f-average.png" style="width:85%" />
						<div><b>Figure</b>: Composition of $M$ levels of aggregated $f$-averages. The operation depicted
							with the sign $\|$ represents a concatenation.
						</div>
					</div>
					<table class="notations_table" style="margin-top: 4%;">
						<tr>
							<td>$\forall j \in \{1,\cdots,J\}$ and</td>
							<td>$\boldsymbol{x_{m,j}}$:</td>
							<td>$\in \mathbb{R}^{KN}$, vertical concatenation of the inputs $(x_k)_{1 \le k \le K}$</td>
						</tr>
						<tr>
							<td>$\forall m \in \{1,\cdots,M\}$,</td>
							<td>$\boldsymbol{f_{m,j}}$:</td>
							<td>activation function operating componentwise from $[0, +\infty[^{KN}$ to $\mathbb{R}^{KN}$</td>
						</tr>
						<tr>
							<td></td>
							<td>$W_{m,j}$:</td>
							<td>$\in [0, + \infty[^{N\times KN}$ such that $W1\kern-0.25em\text{l}_{KN} =
								1\kern-0.25em\text{l}_{N}$
							</td>
						</tr>
						<tr>
							<td></td>
							<td>$f^{-1}_{m,j}$:</td>
							<td>inverse activation function operating componentwise from $\mathbb{R}^{N}$ to $[0,
								+\infty[^{N}$
							</td>
						</tr>
						<tr>
							<td></td>
							<td>$A$:</td>
							<td>$\in \mathbb{R}^{N \times NJ}$</td>
						</tr>
					</table>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="fade-in slide-out" class="section_slide">
					<h1>Part I - Ensemble learning</h1>
					<hr />
					<div style="width: 100%;">
						<div style="float:left; width: 50%;">
							<h2 style="opacity: 0;">-</h2>
							<ol>
								<span class="semi-fade">
									<li>
										Introduction
										<ol>
											<li>Wisdom of the crowd</li>
											<li>Ensemble learning</li>
											<li>Generalised mean</li>
										</ol>
									</li>
									<li>
										Proposed model
										<ol>
											<li>$f$-average</li>
											<li>Aggregated $f$-averages</li>
											<li>Stacked aggregated $f$-averages</li>
										</ol>
									</li>
								</span>
								<li>
									Application to Few-Shot Class Incremental Learning
									<ol>
										<li>Motivation & problem setting</li>
										<li>Proposed approach</li>
										<li>Comparative results</li>
									</ol>
								</li>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>FSCIL: motivation</h1>
					<hr />
					<div class="centred_img">
						<img src="res/smartalbum.png" style="width: 45%" />
						<div>FSCIL use case: AI-powered smart album.</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>FSCIL: problem setting</h1>
					<hr />
					<table class="centred_table">
						<thead>
							<tr>
								<th>Dataset</th>
								<th>$K$</th>
								<th>$n_\text{class\_base}$</th>
								<th>$n_\text{class}$</th>
								<th>$n_\text{way}$</th>
								<th>$n_\text{shots}$</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>mini-ImageNet</td>
								<td>9</td>
								<td>60</td>
								<td>100</td>
								<td>5</td>
								<td>5</td>
							</tr>
							<tr>
								<td>FGVC-Aircraft</td>
								<td>10</td>
								<td>50</td>
								<td>100</td>
								<td>5</td>
								<td>5</td>
							</tr>
							<tr>
								<td>tiered-ImageNet</td>
								<td>10</td>
								<td>100</td>
								<td>200</td>
								<td>10</td>
								<td>5</td>
							</tr>
							<tr>
								<td>CUB-200</td>
								<td>10</td>
								<td>100</td>
								<td>200</td>
								<td>10</td>
								<td>5</td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title"><b>Table</b>: FSCIL setting values for each dataset</div>
					<table class="centred_table" style="margin-top: 7%;">
						<thead>
							<tr>
								<td><b>Session</b></td>
								<td><b>Training classes</b></td>
								<td><b>Test classes</b></td>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>0</td>
								<td>{0, … , 59}</td>
								<td>{0, … , 59}</td>
							</tr>
							<tr>
								<td>1</td>
								<td>{60, … , 64}</td>
								<td>{0, … , 64}</td>
							</tr>
							<tr>
								<td>$\vdots$</td>
								<td>$\vdots$</td>
								<td>$\vdots$</td>
							</tr>
							<tr>
								<td>9</td>
								<td>{95, … , 99}</td>
								<td>{0, … , 99}</td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title"><b>Table</b>: Example of a setting configuration (mini-ImageNet)</div>
					<div style="margin-top: 7%;"><b>Goal 🎯</b>: learn new classes, using only a few training samples and
						without forgetting previous classes.
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>FSCIL: proposed approach - pipeline</h1>
					<hr />
					<div class="centred_img">
						<img src="res/fscil_afa_pipeline.png" style="width: 45%" />
						<div style="width: 70%;"><b>Figure</b>: scheme of our proposed approach. Each session is considered
							as a few-shot task in order to build a set of models that are then ensembled using our AFA
							method.
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>FSCIL: proposed approach - weak learners training</h1>
					<hr />
					<h2>$H_{W_k}$: nearest-neighbor classifier [3]</h2>
					<div class="theorem">
						<div class="theorem_title">Mean centroid of each training class</div>
						\[
							(\forall k \in \{1, \dots, K\}) (\forall c \in C_k^\text{train}) \quad \bar{z_c} = \frac{1}{n_{\text{shots}}}\sum_{n = 1}^{n_{\text{shots}}} z_{n,c}
						\]
					</div>
					where $z$ is the $\ell_2$-normalised feature vector of a sample image $i$
					\[
						z = \frac{f_\theta(i)}{\|f_\theta(i)\|_2}
					\]
					<div class="theorem" style="margin-top: 2%; padding-bottom: 0.1em">
						<div class="theorem_title">Estimated class $\widehat{c}(i)$ of a test image $i$</div>
						\[
							\widehat{c}(i) = \argmin_{c \in C_k^\text{test}} \|z - \bar{z_c}\|^2
						\]
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>FSCIL: proposed approach - padding</h1>
					<hr />
					<div style="width: 100%; margin-top: 3%;">
						<div style="float:left; width: 60%; border-right: 1px solid #254456;">
							<div class="centred_img">
								<img src="res/fscil_afa_pipeline.png" style="width: 85%" />
							</div>
						</div>
						<div style="float:right; width: 39%; margin-top: 13%; text-align: left;">
							\[\begin{align*}
								x'_i & = \begin{cases}
									\frac{1}{2} (\frac{N_k}{N_K} + 1) x_i, & \qquad \; \text{if} \max\limits_{1 \le i \le N_k}x_i \geq t\\
									\frac{1}{2} (\frac{N_k}{N_K}) x_i, & \qquad \; \text{otherwise}
								\end{cases} \\ ~ \\
								p'_i & = \begin{cases}
									(1 - \frac{1}{2} (\frac{N_k}{N_K} + 1)) p_i, & \text{if} \max\limits_{1 \le i \le N_k} x_i \ge t\\
									(1 - \frac{1}{2} (\frac{N_k}{N_K})) p_i, & \text{otherwise}
								\end{cases}
							\end{align*}\]
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
				</section>
				<section data-auto-animate class="section_slide">
					<h1>FSCIL: Comparatives results - ensemble learning</h1>
					<hr data-id="header_hr"/>
					<div style="width: 100%; ">
						<div style="float:left; width: 30%;">
							<ul style="margin-top: 30%;">
								<li class="fragment fade-down"><b>Averages</b>
									<ul>
										<li>Arithmetic</li>
										<li>Geometric</li>
										<li>Harmonic</li>
										<li>Majority voting</li>
									</ul>
								</li>
								<li class="fragment fade-down" style="padding-top: 1.5em"><b>Stacked NN models</b>
									<ul>
										<li>Shallow NN</li>
										<li>Deep NN</li>
										<li>Weighted average NN</li>
										<li>AFA</li>
									</ul>
								</li>
							</ul>
						</div>
						<div class="fragment fade-in" style="float:right; width: 69%; border-left: 1px solid #254456;">
							<div class="centred_img">
								<img data-id="perf_graph" src="res/fscil_ensemble_perf.png" style="width: 65%" />
								<div style="width: 70%;"><b></b>
								</div>
							</div>
						</div>
					</div>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
				</section>
				<section data-auto-animate class="section_slide">
					<h1>FSCIL: Comparatives results - state of the art</h1>
					<hr data-id="header_hr"/>
					<div class="centred_img">
						<img data-id="perf_graph" src="res/fscil_ensemble_perf_sota.png" style="width: 50%" />
						<div style="width: 70%;"><b></b>
						</div>
					</div>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Part I: Conclusion</h1>
					<hr />
					<ul>
						<li style="margin-top: 13%;">General framework for describing averaging rules in ensemble learning</li>
							<ul>
								<li>Focus on output fusion</li>
							</ul>
						<li style="padding-top: 2em;">Optimal ensembling through training of an interpretable neural network architecture
							<ul>
								<li>$f$-average: neural network modelling an average</li>
								<li>Aggregating multiple $f$-averages</li>
								<li>Stacking multiple aggregated $f$-averages</li>
							</ul>
						</li>
						<li style="padding-top: 2em;">Application to few-shot class incremental learning
							<ul>
								<li>Best performance among ensemble learning methods</li>
								<li>Comparable performance with specialised state-of-the-art methods</li>
							</ul>
						</li>
					</ul>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
				</section>
				<section data-transition="fade-in slide-out" class="section_slide">
					<h1>Part II - Deep learning for photorefraction</h1>
					<hr />
					<div style="width: 100%;">
						<div style="float:left; width: 50%;">
							<ol>
								<li>
									Introduction
									<ol>
										<li>Context</li>
										<li>Objectives</li>
										<li>Eye refractive error</li>
									</ol>
								</li>
								<span class="semi-fade">
									<li>
										Proposed pipeline
										<ol>
											<li>Photorefraction principle</li>
											<li>Essilor capture device</li>
											<li>Objectives</li>
											<li>Datasets, CNN architecture & training</li>
											<li>End-to-end pipeline</li>
										</ol>
									</li>
									<li>
										Implementation & performance
										<ol>
											<li>Regression models benchmark</li>
											<li>Real versus synthetic data</li>
											<li>Performance comparison with other device</li>
											<li>Implementation performance</li>
											<li>Ensembling regression models</li>
										</ol>
									</li>
									<li>
										Robustness analysis
										<ol>
											<li>Introduction on Lipschitz constant</li>
											<li>Improve robustness with a denoiser</li>
											<li>Controlled Lipschitz constant training</li>
										</ol>
									</li>
								</span>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-auto-animate class="section_slide">
					<h1>Introduction: context</h1>
					<hr data-id="header_hr"/>
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/blindness_vietnam.jpg" style="width:70%" />
					</div>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-auto-animate class="section_slide">
					<h1>Introduction: context</h1>
					<hr data-id="header_hr"/>
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/blindness_vietnam.jpg" style="width:50%" />
					</div>
					<div style="margin-top: 5%;">
						<ul>
							<li class="fragment fade-down" style="padding-top: .5em;">1 billion people with vision impairment unadressed</li>
							<li class="fragment fade-down" style="padding-top: .5em;">uncorrected refractive errors: a leading cause of vision impairment</li>
						</ul>
					</div>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Introduction: context</h1>
					<hr />
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/myopia_projection.png" style="width: 70%" />
						<div><b>Source</b>: Global prevalence of myopia and high myopia and temporal trends from 2000 through 2050 [9]</div>
					</div>
					<div style="margin-top: 4%;">
						<ul>
							<li class="fragment fade-down">Myopia might affect almost 50 % of the population by 2050</li>
							<li class="fragment fade-down" style="padding-top: 1em;">A critical challenge for developing countries</li>
						</ul>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Introduction: project objectives</h1>
					<hr data-id="header_hr"/>
					<div style="margin-top: 13%;">
						Design a screening tool <br />
						<ul>
							<li style="padding-top: 1em;">accessible (easy, comfortable to use even for non-experts)</li>
							<li style="padding-top: 1em;">affordable</li>
							<li style="padding-top: 1em;">with competitive performance</li>
							<li style="padding-top: 1em;">reliable</li>
						</ul>
					</div>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Introduction: eye refractive error</h1>
					<hr />
					<div class="centred_img">
						<img src="res/zb/eye_anatomy.png" style="width: 40%" />
					</div>
					<table style="margin-top: 2%;">
						<tr>
							<td style="text-align: center;">Sphere</td>
							<td style="text-align: center;">Cylinder</td>
							<td style="text-align: center;">Axis</td>
						</tr>
						<tr>
							<td style="text-align: center;">$[-13, 10]$ (dioptre)</td>
							<td style="text-align: center;">$[-6, 0]$ (dioptre)</td>
							<td style="text-align: center;">$[0, 180]$ (degree)</td>
						</tr>
					</table>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="fade-in slide-out" class="section_slide">
					<h1>Part II - Deep learning for photorefraction</h1>
					<hr />
					<div style="width: 100%;">
						<div style="float:left; width: 50%;">
							<ol>
								<span class="semi-fade">
									<li>
										Introduction
										<ol>
											<li>Context</li>
											<li>Objectives</li>
											<li>Eye refractive error</li>
										</ol>
									</li>
								</span>
								<li>
									Proposed pipeline
									<ol>
										<li>Photorefraction principle</li>
										<li>Essilor capture device</li>
										<li>Objectives</li>
										<li>Datasets, CNN architecture & training</li>
										<li>End-to-end pipeline</li>
									</ol>
								</li>
								<span class="semi-fade">
									<li>
										Implementation & performance
										<ol>
											<li>Regression models benchmark</li>
											<li>Real versus synthetic data</li>
											<li>Performance comparison with other device</li>
											<li>Implementation performance</li>
											<li>Ensembling regression models</li>
										</ol>
									</li>
									<li>
										Robustness analysis
										<ol>
											<li>Introduction on Lipschitz constant</li>
											<li>Improve robustness with a denoiser</li>
											<li>Controlled Lipschitz constant training</li>
										</ol>
									</li>
								</span>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Proposed pipeline: photorefraction principle</h1>
					<hr />
					<div class="centred_img">
						<img src="res/zb/photorefraction_scheme.png" style="width: 50%" />
					</div>
					<table class="notations_table" style="margin: 4% auto;">
						<tr>
							<td></td>
							<td>$O$:</td>
							<td>center of the camera aperture</td>
							<td style="padding-left: 7em;">$e$:</td>
							<td>eccentricity</td>
						</tr>
						<tr>
							<td></td>
							<td>$r_c$:</td>
							<td>radius of the camera aperture</td>
							<td>$AB$:</td>
							<td>spot on the retina</td>
						</tr>
						<tr>
							<td></td>
							<td>$C$:</td>
							<td>camera aperture extremity</td>
							<td>$r$:</td>
							<td>pupil radius</td>
						</tr>
						<tr>
							<td></td>
							<td>$L$:</td>
							<td>light point source</td>
							<td>$d$:</td>
							<td>camera to eye distance</td>
						</tr>
					</table>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Proposed pipeline: photorefraction principle</h1>
					<hr />
					<div style="width: 100%; margin-top: 3%;">
						<div style="float:left; width: 35%;">
							<div class="centred_img">
								<img src="res/res_deep_photorefraction/eye_scheme.png" style="width: 85%; margin: 25% auto;"/>
							</div>
						</div>
						<div style="float:right; width: 64%; margin-top: 13%; text-align: left;">
							The light pattern whose size $s$, shape and orientation depend on:
							<ul>
								<li style="padding-top: 1em;">the subject's ametropia (eg.myopia),</li>
								<li style="padding-top: 1em;">the light source position wrt. the camera aperture,</li>
								<li style="padding-top: 1em;">the measurement distance $d$.</li>
							</ul>
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024
						</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Proposed pipeline: Essilor photorefraction device</h1>
					<hr />
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/pipeline_archi_1.png" style="width: 35%;"/>
					</div>
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/captured_eye.png" style="width: 90%; margin-top: 3%;"/>
					</div>
					<div style="width: fit-content; margin: 3% auto;">Capture process < 400 ms</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Proposed pipeline: datasets</h1>
					<hr />
					<table id="table_datasets">
						<tr>
							<td><h3>Datasets</h3></td>
							<td></td>
						</tr>
						<tr>
							<td><h4>Synthetic:</h4></td>
							<td><h4>Real:</h4></td>
						</tr>
						<tr>
							<td>+ Free / unlimited</td>
							<td>− Expensive / limited</td>
						</tr>
						<tr>
							<td>+ 1M samples</td>
							<td>− 3000 samples</td>
						</tr>
						<tr>
							<td>− Artificial</td>
							<td>+ Genuine</td>
						</tr>
						<tr>
							<td>+ Selected distribution</td>
							<td>− Uncontrolled distribution</td>
						</tr>
					</table>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-auto-animate class="section_slide">
					<h1>Proposed pipeline: neural network architecture</h1>
					<hr data-id="hr_header"/>
					<table id="target_table" style="margin-top: 2%;">
						<tr>
							<td>Sphere</td>
							<td>Cylinder</td>
							<td>Axis</td>
							<td>➡️</td>
							<td>$M$</td>
							<td>$J_0$</td>
							<td>$J_{45}$</td>
						</tr>
						<tr>
							<td>$[-13, 10]$ (dioptre)</td>
							<td>$[-6, 0]$ (dioptre)</td>
							<td>$[0, 180]$ (degree)</td>
							<td></td>
							<td>$[-14, 10]$ (dioptre)</td>
							<td>$[-3, 3]$ (dioptre)</td>
							<td style="text-align: center;">$[-2, 3]$ (dioptre)</td>
						</tr>
					</table>
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/pipeline_archi_2.png" style="width: 75%; margin-top: 5%;"/>
					</div>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-auto-animate class="section_slide">
					<h1>Proposed pipeline: neural network architecture</h1>
					<hr data-id="hr_header"/>
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/pipeline_archi_2.png" style="width: 65%; margin-top: 2%;"/>
					</div>
					<ul class="fragment fade-down" id="cnn_models_list" style="margin-top: 2%;">
						<li>regression: from images to a parameter value</li>
						<li>a different set of weights for each target parameter</li>
						<li>lightweight: running on a embedded software</li>
						<li>fast inference for user experience</li>
						<li>without compromising performance</li>
					</ul>
					<div class="footer" data-id="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Proposed pipeline: complete pipeline</h1>
					<hr />
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/pipeline_archi.png" style="width: 80%; margin-top: 2%;"/>
					</div>
					<div style="width: 100%; margin-top: 3%;">
						<div style="float:left; width: 50%;">
							A smartphone coordinates the pipeline <br />
							<ul id="cnn_models_list">
								<li>is the user interface through an app</li>
								<li>triggers and operates the capture device</li>
								<li>runs the image processing algorithm</li>
								<li>runs the neural network inference</li>
								<li>determines and displays Sphere, Cylinder and Axis</li>
							</ul>
						</div>
						<div style="float:right; width: 50%;">
							<!-- <div class="centred_img">
								<img src="res/res_deep_photorefraction/device.png" style="width: 50%;"/>
							</div> -->
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="fade-in slide-out" class="section_slide">
					<h1>Part II - Deep learning for photorefraction</h1>
					<hr />
					<div style="width: 100%;">
						<div style="float:left; width: 50%;">
							<ol>
								<span class="semi-fade">
									<li>
										Introduction
										<ol>
											<li>Context</li>
											<li>Objectives</li>
											<li>Eye refractive error</li>
										</ol>
									</li>
									<li>
										Proposed pipeline
										<ol>
											<li>Photorefraction principle</li>
											<li>Essilor capture device</li>
											<li>Objectives</li>
											<li>Datasets, CNN architecture & training</li>
											<li>End-to-end pipeline</li>
										</ol>
									</li>
								</span>
								<li>
									Implementation & performance
									<ol>
										<li>Regression models benchmark</li>
										<li>Real versus synthetic data</li>
										<li>Performance comparison with other device</li>
										<li>Implementation performance</li>
										<li>Ensembling regression models</li>
									</ol>
								</li>
								<span class="semi-fade">
									<li>
										Robustness analysis
										<ol>
											<li>Introduction on Lipschitz constant</li>
											<li>Improve robustness with a denoiser</li>
											<li>Controlled Lipschitz constant training</li>
										</ol>
									</li>
								</span>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Performance: regression models benchmark</h1>
					<hr />
					<table class="centred_table" id="regression_table" style="margin-top: 10%;">
						<thead>
							<tr>
								<th></th>
								<th></th>
								<th>$M$</th>
								<th></th>
								<th></th>
								<th>$J_0$</th>
								<th></th>
								<th></th>
								<th>$J_{45}$</th>
								<th></th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>Error</td>
								<td>BSM</td>
								<td>FC</td>
								<td>CNN</td>
								<td>BSM</td>
								<td>FC</td>
								<td>CNN</td>
								<td>BSM</td>
								<td>FC</td>
								<td>CNN</td>
							</tr>
							<tr>
								<td>min</td>
								<td>-4.66</td>
								<td>-2.31</td>
								<td><b>-2.16</b></td>
								<td>-2.82</td>
								<td>-1.22</td>
								<td><b>-0.97</b></td>
								<td>-2.56</td>
								<td><b>-0.83</b></td>
								<td>-1.17</td>
							</tr>
							<tr>
								<td>mean</td>
								<td>0.05</td>
								<td><b>0.02</b></td>
								<td>-0.03</td>
								<td>-0.04</td>
								<td><b>0.01</b></td>
								<td>0.02</td>
								<td>0.01</td>
								<td>0.01</td>
								<td>-0.01</td>
							</tr>
							<tr>
								<td>std</td>
								<td>0.75</td>
								<td>0.60</td>
								<td><b>0.56</b></td>
								<td>0.43</td>
								<td>0.28</td>
								<td><b>0.26</b></td>
								<td>0.32</td>
								<td>0.23</td>
								<td><b>0.22</b></td>
							</tr>
							<tr>
								<td>max</td>
								<td>5.73</td>
								<td>2.86</td>
								<td><b>2.50</b></td>
								<td>2.62</td>
								<td>1.01</td>
								<td><b>0.88</b></td>
								<td>2.36</td>
								<td>1.13</td>
								<td><b>0.80</b></td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title">
						<b>Table</b>: Regression models comparison on the dataset of real images
					</div>
					<ul id="cnn_models_list" style="margin-top: 5%;">
						<li>Since a biased model can easily be corrected by offsetting predictions, our main evaluation criterion is the error <b>standard deviation</b>.</li>
						<li>To better assess these results, recall that glasses are prescribed with a precision of 0.25 D.</li>
					</ul>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Performance: synthetic vs. real data</h1>
					<hr />
					<table id="synth_real_data_table" style="margin-top: 10%;">
						<thead>
							<tr>
								<th>Train</th>
								<th>Test</th>
								<th>$M$</th>
								<th>$J_0$</th>
								<th>$J_{45}$</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td rowspan="2" style="vertical-align : middle;">Synthetic</td>
								<td>Synthetic</td>
								<td>0.10</td>
								<td>0.07</td>
								<td>0.07</td>
							</tr>
							<tr>
								<td>Real</td>
								<td>0.70</td>
								<td>0.33</td>
								<td>0.32</td>
							</tr>
							<tr>
								<td>Real</td>
								<td>Real</td>
								<td><b>0.56</b></td>
								<td><b>0.26</b></td>
								<td><b>0.22</b></td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title">
						<b>Table</b>: CNN prediction error standard deviation wrt. ground truth (in D.) on synthetic and real data.
					</div>
					<ul id="cnn_models_list" style="margin-top: 5%;">
						<li>Synthetic images are not sufficient by themselves</li>
						<li>Training on real images yields best results</li>
						<li>Synthetic images are still useful to test on a wider range of parameters</li>
						<li>Mixed data training yields even better results</li>
					</ul>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Performance: comparison with competing device</h1>
					<hr />
					<table id="device_comp_table" style="margin-top: 10%;">
						<thead>
							<tr>
								<th></th>
								<th colspan="2">2WIN [11]</th>
								<th colspan="2">2WIN [12]</th>
								<th colspan="2">Ours</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td></td>
								<td>mean</td>
								<td>std</td>
								<td>mean</td>
								<td>std</td>
								<td>mean</td>
								<td>std</td>
							</tr>
							<tr>
								<td>$M$</td>
								<td>0.41</td>
								<td>1.37</td>
								<td>1.25</td>
								<td>1.15</td>
								<td><b>-0.03</b></td>
								<td><b>0.56</b></td>
							</tr>
							<tr>
								<td>$J_0$</td>
								<td>-0.08</td>
								<td>0.35</td>
								<td>0.19</td>
								<td>0.45</td>
								<td><b>0.02</b></td>
								<td><b>0.26</b></td>
							</tr>
							<tr>
								<td>$J_{45}$</td>
								<td><b>0.00</b></td>
								<td>0.25</td>
								<td>-0.05</td>
								<td><b>0.15</b></td>
								<td>-0.01</td>
								<td>0.22</td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title">
						<b>Table</b>: CNN prediction error standard deviation wrt. ground truth (in D.) on synthetic and real data.
					</div>
					<ul id="cnn_models_list" style="margin-top: 5%;">
						<li>Best results on $M$ and $J_0$</li>
						<li>Competitive on $J_{45}$</li>
					</ul>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Performance: implementation performance</h1>
					<hr />
					<ul id="cnn_models_list" style="margin-top: 10%;">
						<li>offline training
							<ul>
								<li>Python Tensorflow</li>
								<li>Nvidia RTX A6000 GPU</li>
								<li>~50 hours per model</li>
							</ul>
						</li>
						<li>smartphone embedded software: ONNX model (ran in C++)</li>
						<li>inference: 33 ms per eye per target (200 ms in total)</li>
						<li>130 MB of storage</li>
					</ul>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Performance: improvement with ensemble learning</h1>
					<hr />
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/ensembling_regression.png" style="width: 75%;"/>
					</div>
					<table class="centred_table fragment fade-down" data-fragment-index="1" style="margin-top: 3%;">
						<thead>
							<tr>
								<th></th>
								<th><b>CNN (mixed)</b></th>
								<th><b>CNN (real)</b></th>
								<th><b>$k$-NN</b></th>
								<th><b>AFA</b></th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>$M$</td>
								<td>0.54</td>
								<td>0.56</td>
								<td>0.57</td>
								<td><b>0.49</b></td>
							</tr>
							<tr>
								<td>$J_0$</td>
								<td>0.28</td>
								<td>0.28</td>
								<td>0.33</td>
								<td><b>0.26</b></td>
							</tr>
							<tr>
								<td>$J_{45}$</td>
								<td>0.21</td>
								<td>0.25</td>
								<td>0.24</td>
								<td><b>0.19</b></td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title fragment fade-down" data-fragment-index="1"><b>Table</b>: Error standard deviation comparison between weak learners and the ensemble learning method</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="fade-in slide-out" class="section_slide">
					<h1>Part II - Deep learning for photorefraction</h1>
					<hr />
					<div style="width: 100%;">
						<div style="float:left; width: 50%;">
							<ol>
								<span class="semi-fade">
									<li>
										Introduction
										<ol>
											<li>Context</li>
											<li>Objectives</li>
											<li>Eye refractive error</li>
										</ol>
									</li>
									<li>
										Proposed pipeline
										<ol>
											<li>Photorefraction principle</li>
											<li>Essilor capture device</li>
											<li>Objectives</li>
											<li>Datasets, CNN architecture & training</li>
											<li>End-to-end pipeline</li>
										</ol>
									</li>
									<li>
										Implementation & performance
										<ol>
											<li>Regression models benchmark</li>
											<li>Real versus synthetic data</li>
											<li>Performance comparison with other device</li>
											<li>Implementation performance</li>
											<li>Ensembling regression models</li>
										</ol>
									</li>
								</span>
								<li>
									Robustness analysis
									<ol>
										<li>Introduction on Lipschitz constant</li>
										<li>Improving robustness with a denoiser</li>
										<li>Controlled Lipschitz constant training</li>
									</ol>
								</li>
							</ol>
						</div>
					</div>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Robustness: introduction</h1>
					<hr />
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/pipeline_archi_2.png" style="width: 80%;"/>
					</div>
					<table class="notations_table">
						<tr>
							<td></td>
							<td>$f_{\theta_j}$:</td>
							<td>trained neural network model</td>
						</tr>
						<tr>
							<td></td>
							<td>$x$:</td>
							<td>input tensor of 12 images</td>
						</tr>
					</table>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Robustness: introduction</h1>
					<hr />
					<div>
						Assuming a small enough input error, and differentiable $f_{\theta_j}$ at $x$,<br />
						<div class="theorem">
							<div class="theorem_title">First-order approximation</div>
							\[
								f_{\theta_j}(x+b) \simeq f_{\theta_j}(x) + \big({\color{teal}\nabla  f_{\theta_j}(x)}\big)^\top b
							\]
						</div>
						Assuming zero-mean Gaussian noise $b$ with covariance matrix $\Lambda_b$, <b>approximation of the output error</b> $e= f_{\theta_j}(x+b)-f_{\theta_j}(x)$, by a zero-mean Gaussian with variance:
						<div class="theorem">
							<div class="theorem_title">Variance of the output error approximation</div>
							\[
							\sigma_e^2(x) = \big({\color{teal}\nabla f_{\theta_j}(x)}\big)^\top \Lambda_b {\color{teal}\nabla f_{\theta_j}(x)}
							\]
						</div>
						<table class="notations_table" style="margin-top: 4%;">
							<tr>
								<td>where</td>
								<td>${\color{teal}\nabla f_{\theta_j}(x)}$:</td>
								<td>network gradient at $x$, easily computed by backpropagation</td>
							</tr>
							<tr>
								<td></td>
								<td>$\Lambda_b$:</td>
								<td>noise covariance, e.g. diagonal matrix with entries $\sigma_b^2$ in the pupil area, and zero elsewhere</td>
							</tr>
						</table>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Robustness: local Lipschitz constant</h1>
					<hr />
					<div class="theorem">
						<div class="theorem_title">Weighted local Lispchitz constant at point $x$ [13]</div>
						\[
							\sigma_e(x)/\sigma_b
						\]
					</div>
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/lipschitz/cnn_norm1_mask.png" style="width: 80%;"/>
						<div><b>Figure</b>: Histograms of weighted local Lipschitz constants computed with $L_2$ norm on the CNN</div>
					</div>
					<div style="margin-top: 3%;">Reasonably low values of the local weighted Lipschitz constant are a strong indicator of the <b>robustness of the network</b> [14].</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Robustness: improving robustness with a denoiser</h1>
					<hr />
					The application of a denoiser (pretrained DRUnet [15]) at test time could help with the robustness [16].
					<div class="centred_img" style="margin-top: 2%;">
						<img src="res/res_deep_photorefraction/lipschitz/denoised.png" style="width: 80%;"/>
						<div><b>Figure</b>: Example of a denoised pupil image</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Robustness: improving robustness with a denoiser</h1>
					<hr />
					<table class="centred_table">
						<tr>
							<td></td>
							<td colspan="2">$M$</td>
							<td colspan="2">$J_0$</td>
							<td colspan="2">$J_{45}$</td>
						</tr>
						<tbody>
							<tr style="border-bottom: 1px solid #254456;">
								<td></td>
								<td>Raw</td>
								<td>Denoised</td>
								<td>Raw</td>
								<td>Denoised</td>
								<td>Raw</td>
								<td>Denoised</td>
							</tr>
							<tr>
								<td>mean</td>
								<td><b>-0.04</b></td>
								<td>0.29</td>
								<td><b>-0.02</b></td>
								<td>0.03</td>
								<td><b>0.01</b></td>
								<td>0.05</td>
							</tr>
							<tr>
								<td>std</td>
								<td><b>0.56</b></td>
								<td>0.75</td>
								<td><b>0.27</b></td>
								<td>0.33</td>
								<td><b>0.22</b></td>
								<td>0.29</td>
							</tr>
							<tr>
								<td>min</td>
								<td>-2.17</td>
								<td><b>-1.91</b></td>
								<td><b>-0.89</b></td>
								<td>-1.31</td>
								<td><b>-0.81</b></td>
								<td>-0.94</td>
							</tr>
							<tr>
								<td>max</td>
								<td><b>2.50</b></td>
								<td>2.92</td>
								<td><b>0.98</b></td>
								<td>1.34</td>
								<td><b>1.18</b></td>
								<td>1.69</td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title"><b>Table</b>: Performance comparison between original and denoised images</div>
					<table class="centred_table fragment fade-up" data-fragment-index="1" style="margin-top: 5%;">
						<tr>
							<td></td>
							<td colspan="2">$M$</td>
							<td colspan="2">$J_0$</td>
							<td colspan="2">$J_{45}$</td>
						</tr>
						<tbody>
							<tr style="border-bottom: 1px solid #254456;">
								<td></td>
								<td>Original</td>
								<td>Denoised</td>
								<td>Original</td>
								<td>Denoised</td>
								<td>Original</td>
								<td>Denoised</td>
							</tr>
							<tr>
								<td>count</td>
								<td>906</td>
								<td>906</td>
								<td>906</td>
								<td>906</td>
								<td>906</td>
								<td>906</td>
							</tr>
							<tr>
								<td>mean</td>
								<td>0.61</td>
								<td><b>0.54</b></td>
								<td>1.47</td>
								<td><b>1.28</b></td>
								<td>2.04</td>
								<td><b>1.46</b></td>
							</tr>
							<tr>
								<td>std</td>
								<td><b>0.36</b></td>
								<td>0.41</td>
								<td><b>1.07</b></td>
								<td>1.13</td>
								<td>1.66</td>
								<td><b>1.10</b></td>
							</tr>
							<tr>
								<td>min</td>
								<td>0.05</td>
								<td><b>0.03</b></td>
								<td>0.08</td>
								<td>0.08</td>
								<td>0.05</td>
								<td><b>0.04</b></td>
							</tr>
							<tr>
								<td>25%</td>
								<td>0.34</td>
								<td><b>0.26</b></td>
								<td>0.70</td>
								<td><b>0.57</b></td>
								<td>0.89</td>
								<td><b>0.72</b></td>
							</tr>
							<tr>
								<td>median</td>
								<td>0.53</td>
								<td><b>0.42</b></td>
								<td>1.21</td>
								<td><b>0.97</b></td>
								<td>1.56</td>
								<td><b>1.12</b></td>
							</tr>
							<tr>
								<td>75%</td>
								<td>0.79</td>
								<td><b>0.69</b></td>
								<td>1.95</td>
								<td><b>1.61</b></td>
								<td>2.63</td>
								<td><b>1.87</b></td>
							</tr>
							<tr>
								<td>max</td>
								<td><b>2.45</b></td>
								<td>3.22</td>
								<td><b>7.98</b></td>
								<td>9.11</td>
								<td>11.34</td>
								<td><b>6.81</b></td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title fragment fade-up" data-fragment-index="1"><b>Table</b>: Weighted local Lipschitz constants comparison between original and denoised images ($L_2$ norm)</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Robustness: controlled Lipschitz constant training</h1>
					<hr />
					Using the deel-lip library [17],
					<div class="centred_img" style="margin-top: 2%;">
						<img src="res/res_deep_photorefraction/lipschitz/perf.png" style="width:95%" />
						<div>Error standard deviation of a model trained with deel-lip in function of $K$</div>
					</div>
					<div style="margin-top: 5%;">
						The lower the Lipschitz constant, the greater the constraints. <br />
						➡️ balance robustness and performance
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section class="section_slide">
					<h1>Robustness: controlled Lipschitz constant training</h1>
					<hr />
					<div style="margin-top: 2%;">Using a CNN trained with a Lipschitz constant below 4 (4-LCNN),</div>
					<table class="centred_table" style="margin-top: 10%;">
						<tr>
							<th></th>
							<th colspan="2">$M$</th>
							<th colspan="2">$J_0$</th>
							<th colspan="2">$J_{45}$</th>
						</tr>
						<tbody>
							<tr style="border-bottom: 1px solid #254456;">
								<td></td>
								<td>CNN</td>
								<td>4-LCNN</td>
								<td>CNN</td>
								<td>4-LCNN</td>
								<td>CNN</td>
								<td>4-LCNN</td>
							</tr>
							<tr>
								<td>mean</td>
								<td>-0.04</td>
								<td>0.06</td>
								<td>-0.02</td>
								<td>0.05</td>
								<td>0.01</td>
								<td>0.01</td>
							</tr>
							<tr>
								<td>std</td>
								<td>0.56</td>
								<td>0.70</td>
								<td>0.27</td>
								<td>0.37</td>
								<td>0.22</td>
								<td>0.24</td>
							</tr>
							<tr>
								<td>min</td>
								<td>-2.17</td>
								<td>-2.63</td>
								<td>-0.89</td>
								<td>-1.13</td>
								<td>-0.81</td>
								<td>-0.83</td>
							</tr>
							<tr>
								<td>max</td>
								<td>2.50</td>
								<td>4.23</td>
								<td>0.98</td>
								<td>1.36</td>
								<td>1.18</td>
								<td>0.97</td>
							</tr>
						</tbody>
					</table>
					<div class="centred_table_title"><b>Table</b>: Performance comparison between the 4-LCNN and the standard CNN, on the dataset of real images</div>		
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="slide-in fade-out" class="section_slide">
					<h1>Robustness: controlled Lipschitz constant training</h1>
					<hr />
					<div class="centred_img" style="margin-top: 1%;">
						<img src="res/res_deep_photorefraction/lipschitz/cnn_norm1_mask.png" style="width:70%" />
						<div>Histograms of weighted local Lipschitz constants computed with $L_2$ norm on the standard CNN</div>
					</div>
					<div class="centred_img" style="margin-top: 1%;">
						<img src="res/res_deep_photorefraction/lipschitz/deelcnn_norm1_mask.png" style="width:70%" />
						<div>Histograms of weighted local Lipschitz constants computed with $L_2$ norm on the 4-LCNN</div>
					</div>	
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="fade" class="section_slide">
					<h1>Part II - Conclusion</h1>
					<hr />
					<div class="centred_img">
						<img src="res/res_deep_photorefraction/pipeline_archi.png" style="width: 80%; margin-top: 2%;"/>
					</div>
					<table class="fragment fade-in" data-fragment-index="0" style="margin-top: 3%;">
						<tr>
							<td>Design a screening tool</td>
							<td></td>
							<td></td>
						</tr>
						<tr>
							<td><li>accessible</li></td>
							<td class="fragment fade-right" data-fragment-index="1">➡️</td>
							<td class="fragment fade-right" data-fragment-index="1">smartphone application</td>
						</tr>
						<tr>
							<td><li>affordable</li></td>
							<td class="fragment fade-right" data-fragment-index="1">➡️</td>
							<td class="fragment fade-right" data-fragment-index="1">low-priced device</td>
						</tr>
						<tr>
							<td><li>with competitive performance</li></td>
							<td class="fragment fade-right" data-fragment-index="1">➡️</td>
							<td class="fragment fade-right" data-fragment-index="1">outperforms competition</td>
						</tr>
						<tr>
							<td><li>reliable</li></td>
							<td class="fragment fade-right" data-fragment-index="1">➡️</td>
							<td class="fragment fade-right" data-fragment-index="1">controllable Lipschitz values</td>
						</tr>
					</table>
					<div class=" footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="fade" class="section_slide">
					<h1>Conclusion</h1>
					<div class="header_hr" data-id="header_hr">
						<hr />
					</div>
					<div style="width: 100%;">
						<div style="float:left; width: 50%;" data-id="left_col">
							<h1 class="fragment fade-down" style="font-size: 1.6em;">Part I - Ensemble learning</h1>
							<ul>
								<li class="fragment fade-down">General framework for describing averaging rules in ensemble learning
									<ul>
										<li>Focus on output fusion</li>
									</ul>
								</li>
								<li style="padding-top: 2em;" class="fragment fade-down">Optimal ensembling through training of an interpretable neural network architecture
									<ul>
										<li>$f$-average: neural network modelling an average</li>
										<li>Aggregating multiple $f$-averages</li>
										<li>Stacking multiple aggregated $f$-averages</li>
									</ul>
								</li>
								<li style="padding-top: 2em;" class="fragment fade-down">Application to few-shot class incremental learning
									<ul>
										<li>Best performance among ensemble learning methods</li>
										<li>Comparable performance with specialised state-of-the-art methods</li>
									</ul>
								</li>
							</ul>
						</div>
						<div style=" float:right; width: 50%;">
							<h2 class="fragment fade-down">Part II - Deep learning for photorefraction</h2>
							<ul>
								<li class="fragment fade-down">
									A novel pipeline for photorefraction based on deep learning
									<ul>
										<li>A capture device</li>
										<li>Regression through CNN models</li>
										<li>Smartphone control </li>
									</ul>
								</li>
								<li class="fragment fade-down" style="padding-top: 2em;">
									Performance validation
									<ul>
										<li>Tested on real and synthetic data</li>
										<li>Outperforms competing device</li>
										<li>Performance improvement with ensembling</li>
									</ul>
								</li>
								<li class="fragment fade-down" style="padding-top: 2em;">
									Robustness analysis
									<ul>
										<li>Robustness evaluation using Lipschitz constant</li>
										<li>Methods to control & improve robustness</li>
									</ul>
								</li>
							</ul>
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="fade" class="section_slide">
					<h1>Perspectives</h1>
					<div class="header_hr" data-id="header_hr">
						<hr />
					</div>
					<div style="width: 100%;">
						<div style="float:left; width: 50%;" data-id="left_col">
							<h1 class="fragment fade-down" style="font-size: 1.6em;">Part I - Ensemble learning</h1>
							<ul>
								<li class="fragment fade-down">$f$-average: scenario illustrating its interpretable weights</li>
								<li style="padding-top: 2em;" class="fragment fade-down">FSCIL: open-set recognition methods as weak classifiers</li>
						</div>
						<div style=" float:right; width: 50%;">
							<h2 class="fragment fade-down">Part II - Deep learning for photorefraction</h2>
							<ul>
								<li class="fragment fade-down">Generative model for image to image translation: transform synthetic into real images</li>
								<li class="fragment fade-down" style="padding-top: 2em;">Deep learning-based out-of-range detection tool & invalid capture detection</li>
								<li class="fragment fade-down" style="padding-top: 2em;">New acquisition campaign & industrial deployment</li>
							</ul>
						</div>
					</div>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
				<section data-transition="fade" class="section_slide">
					<h1>References</h1>
					<div class="header_hr" data-id="header_hr">
						<hr />
					</div>
					<table id="ref_table">
						<tr>
							<td></td>
							<td>[1]</td>
							<td>Galton, F. (1907). Vox populi (the wisdom of crowds). <em>Nature</em>, 75(7):450–451.</td>
						</tr>
						<tr>
							<td></td>
							<td>[2]</td>
							<td>Kolmogorov, A. (1930). <em>Mathematics and Mechanics</em>. Kluwer.</td>
						</tr>
						<tr>
							<td></td>
							<td>[3]</td>
							<td>Wang, Y., Chao, W., Weinberger, K. Q., and van der Maaten, L. (2019). Simpleshot: Revisiting nearest-neighbor classification for few-shot learning. DOI: 10.48550/arxiv.1911.04623.</td>
						</tr>
						<tr>
							<td></td>
							<td>[4]</td>
							<td>Castro, F. M., Marín-Jiménez, M. J., Guil, N., Schmid, C., and Alahari, K. (8th September - 14th September 2018). End-to-end incremental learning. In <em>Proceedings of the European Conference on Computer Vision (ECCV)</em>, pages 233–248, Munich, Germany. DOI: 10.1007/978-3-030-01258-8_15.</td>
						</tr>
						<tr>
							<td></td>
							<td>[5]</td>
							<td>Hou, S., Pan, X., Loy, C. C., Wang, Z., and Lin, D. (16th June - 20th June 2019). Learning a unified classifier incrementally via rebalancing. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 831–839, Long Beach, California. DOI: 10.1109/cvpr.2019.00092.</td>
						</tr>
						<tr>
							<td></td>
							<td>[6]</td>
							<td>Zhu, K., Cao, Y., Zhai, W., Cheng, J., and Zha, Z.-J. (19th June - 25th June 2021). Self-promoted prototype refinement for few-shot class-incremental learning. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 6801–6810, (online). DOI: 10.1109/cvpr46437.2021.00673.</td>
						</tr>
						<tr>
							<td></td>
							<td>[7]</td>
							<td>Zhang, C., Song, N., Lin, G., Zheng, Y., Pan, P., and Xu, Y. (19th June - 25th June 2021). Few-shot incremental learning with continually evolved classifiers. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 12455–12464, (online). DOI: 10.1109/cvpr46437.2021.01227.</td>
						</tr>
						<tr>
							<td></td>
							<td>[8]</td>
							<td>Zhou, D.-W., Wang, F.-Y., Ye, H.-J., Ma, L., Pu, S., and Zhan, D.-C. (19th June - 24th June 2022). Forward compatible few-shot class-incremental learning. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 9046–9056, New Orleans, Louisiana. DOI: 10.1109/cvpr52688.2022.00884.</td>
						</tr>
						<tr>
							<td></td>
							<td>[9]</td>
							<td>Holden, B. A., Fricke, T. R., Wilson, D. A., Jong, M., Naidoo, K. S., Sankaridurg, P., Wong, T. Y., Naduvilath, T. J., and Resnikoff, S. (2016). Global prevalence of myopia and high myopia and temporal trends from 2000 through 2050. <em>Ophthalmology</em>, 123(5):1036–1042, DOI: 10.1016/j.ophtha.2016.01.006.</td>
						</tr>
						<tr>
							<td></td>
							<td>[10]</td>
							<td>Gekeler, F., Schaeffel, F., Howland, H. C., and Wattam-Bell, J. (1997). Measurement of astigmatism by automated infrared photoretinoscopy. <em>Optometry and Vision Science</em>, 74(7):472–482, DOI: 10.1097/00006324-199707000-00013.</td>
						</tr>
						<tr>
							<td></td>
							<td>[11]</td>
							<td>Martin, S. J., Htoo, H. E., Hser, N., and Arnold, R. W. (2020). Performance of two photoscreeners enhanced by protective cases. <em>Clinical Ophthalmology</em>, page 1427–1435.</td>
						</tr>
						<tr>
							<td></td>
							<td>[12]</td>
							<td>Kurent, A. (2022). Comparison of the cycloplegic refractive measurements with handheld, table-mounted refractometers and retinoscopy in children. <em>Ophthalmology Journal</em>, 7:200–207, DOI: 10.5603/oj.2022.0028.</td>
						</tr>
						<tr>
							<td></td>
							<td>[13]</td>
							<td>Combettes, P. L. and Pesquet, J.-C. (2020). Lipschitz certificates for neural network structures driven by averaged activation operators. <em>SIAM Journal on Mathematics of Data Science</em>, 2:529–557.</td>
						</tr>
						<tr>
							<td></td>
							<td>[14]</td>
							<td>Gupta, K., Kaakai, F., Pesquet-Popescu, B., Pesquet, J.-C., and Malliaros, F. D. (2022). Multivariate Lipschitz analysis of the stability of neural networks. <em>Frontiers in Signal Processing</em>, 2, ISSN: 2673-8198, DOI: 10.3389/frsip.2022.794469.</td>
						</tr>
						<tr>
							<td></td>
							<td>[15]</td>
							<td>Zhang, K., Zuo, W., Gu, S., and Zhang, L. (21st July - 26th July 2017). Learning deep cnn denoiser prior for image restoration. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 2808–2817, Honolulu, Hawaii. DOI: 10.1109/CVPR.2017.300.</td>
						</tr>
						<tr>
							<td></td>
							<td>[16]</td>
							<td>Aghdam, H. H., Heravi, E. J., and Puig, D. (27th February - 1st March 2017). Increasing the stability of cnns using a denoising layer regularized by local lipschitz constant in roadunderstanding problems. In <em>Proceedings of the International Joint Conference on Computer Vision, Imaging and ComputerGraphics Theory and Applications (VISIGRAPP)</em>, pages 218–225, Porto, Portugal. INSTICC, SciTePress, ISBN: 978-989-758-226-4, DOI: 10.5220/0006123602180225.</td>
						</tr>
						<tr>
							<td></td>
							<td>[17]</td>
							<td>Serrurier, M., Mamalet, F., González-Sanz, A., Boissin, T., Loubes, J.M., and del Barrio, E. (2020). Achieving robustness in classification using optimal transport with hinge regularization. DOI: 10.48550/arxiv.2006.06520.</td>
						</tr>
						
					</table>
					<div class="footer">
						<hr />
						<div>Novel Deep Learning Approaches For Image Analysis | Mathieu Vu | Ph.D. defense - 28th November 2024</div>
					</div>
				</section>
			</div>
		</div>
		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			Reveal.initialize({
				width: '90%',
				height: '90%',
				// width: '100%',
				// height: '100%',
				controls: false,
				hash: true,
				center: false,
				mouseWheel: true,
				slideNumber: 'true',
				plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX],
				pdfSeparateFragments: false,
			});
		</script>
	</body>
</html>
